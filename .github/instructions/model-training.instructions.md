---
description: 'VLA model training: architecture, distributed training, monitoring'
applyTo: '**/*train*.py, **/*model*.py, **/*trainer*.py'
---

# Model Training

Standards for training Vision-Language-Action models.

## Architecture Selection

| Type | Use Case | Key Components |
|------|----------|----------------|
| Transformer (RT-1/2, Octo) | General VLA | Vision + Language + Action decoder |
| Diffusion (pi0, Diffusion Policy) | Precise actions | Diffusion model |
| Flow Matching (pi0) | Efficient sampling | Flow network |
| Generative (OpenVLA, PALM-E) | Large-scale multi-task | LLM + Vision tower |
| RL (PPO, SAC) | Online learning | Policy + Value network |

## Best Practices

| Practice | Implementation | Avoid |
|----------|----------------|-------|
| Gradient clipping | `clip_grad_norm_(params, 1.0)` | No clipping â†’ NaN |
| LR warmup | Linear 2K steps | Full LR â†’ instability |
| Checkpointing | Every N steps + best | Only final |
| Progress tracking | tqdm with ETA | Silent training |

## Code Example

**âœ… Good - With Monitoring:**

```python
from tqdm import tqdm
import torch

def train_epoch(model, loader, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepochï¼Œå¸¦ç›‘æŽ§"""
    model.train()
    pbar = tqdm(loader, desc=f"Epoch {epoch}")
    
    for batch in pbar:
        loss = model(**batch).loss
        
        # æ£€æŸ¥NaN
        if torch.isnan(loss):
            logger.error(f"âš ï¸ NaN at step {step}")
            save_checkpoint("nan_debug.pt")
            raise ValueError("NaN detected")
        
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f"{loss:.4f}",
            'grad': f"{grad_norm:.3f}",
            'lr': f"{optimizer.param_groups[0]['lr']:.2e}"
        })
```

**âŒ Bad - No Monitoring:**

```python
def train(model, data_loader):
    for batch in data_loader:  # æ— è¿›åº¦æ˜¾ç¤º
        loss = model(batch)
        loss.backward()  # æ— æ¢¯åº¦è£å‰ª
        optimizer.step()  # æ— NaNæ£€æŸ¥
```

## File Organization

Multi-file tasks:
```
training_task/
â”œâ”€â”€ README.md          # å¿…é¡»ï¼šä»»åŠ¡è¯´æ˜Ž + é£Žé™©è­¦å‘Š
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ model.py           # æ¨¡åž‹å®šä¹‰
â””â”€â”€ config.yaml        # é…ç½®
```

## README Requirements

```markdown
## âš ï¸ Risk Warnings

### ðŸ”´ Critical
- **GPU Hours**: 48h on 4Ã— A100 (~$200 cloud cost)
- **Checkpoint Overwrite**: Will overwrite existing checkpoints
  - Mitigation: Backup before run

### ðŸŸ¡ High
- **OOM Risk**: Large batch may exceed 24GB VRAM
  - Monitor: GPU memory logged
  - Recovery: Reduce batch size

## Resource Requirements
- Hardware: 4Ã— RTX 3090+ (24GB)
- Disk: 100GB dataset + 500GB checkpoints
- Time: ~48 GPU-hours
```

## Progress Tracking

```python
# å¿…é¡»æ˜¾ç¤ºï¼šè¿›åº¦ + æŒ‡æ ‡ + ETA + èµ„æº
# Epoch 12/100: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 40% | Loss: 0.023 â†“ | ETA: 8h 30m | GPU: 92%
```

## Workflow Checklist

Before training:
- [ ] Dataset validated
- [ ] Architecture configured
- [ ] Hyperparameters logged
- [ ] Checkpoint dir has space
- [ ] README with risk warnings

During:
- [ ] Loss decreasing
- [ ] No NaN values
- [ ] GPU >80% utilized
- [ ] Checkpoints saving

After:
- [ ] Best checkpoint identified
- [ ] Metrics documented
- [ ] Risks encountered logged
