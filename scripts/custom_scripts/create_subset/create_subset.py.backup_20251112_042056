#!/usr/bin/env python3
"""
åˆ›å»º CALVIN æ•°æ®é›†å­é›† - ä½¿ç”¨ HuggingFace ç¼“å­˜ç›®å½•

æ•°æ®å°†ä¿å­˜åœ¨ HuggingFace çš„æ ‡å‡†ç¼“å­˜ç›®å½•ä¸­ï¼š
~/.cache/huggingface/lerobot/

ç›®å½•ç»“æ„ï¼š
~/.cache/huggingface/lerobot/
â”œâ”€â”€ Coil1987121/calvin_lerobot_task_ABCD_D_training/        # å®Œæ•´è®­ç»ƒé›†
â”œâ”€â”€ Coil1987121/calvin_lerobot_task_ABCD_D_validation/      # å®Œæ•´éªŒè¯é›†
â”œâ”€â”€ Coil1987121/calvin_5percent_seed42_training/            # 5% è®­ç»ƒé›†
â”œâ”€â”€ Coil1987121/calvin_5percent_seed42_validation/          # 5% éªŒè¯é›†
â””â”€â”€ ...

ä½¿ç”¨æ–¹æ³•ï¼š
    # åˆ›å»º 5% å­é›†ï¼ˆè®­ç»ƒ+éªŒè¯ï¼‰
    python create_subset_hf_cache.py --create_both

    # åˆ›å»º 10% å­é›†
    python create_subset_hf_cache.py --fraction 0.1 --create_both

    # è‡ªå®šä¹‰å­é›†åç§°
    python create_subset_hf_cache.py \\
        --fraction 0.05 \\
        --subset_suffix my_custom_subset \\
        --create_both

åœ¨è®­ç»ƒé…ç½®ä¸­ä½¿ç”¨ï¼š
    repo_id="Coil1987121/calvin_5percent_seed42_training"
"""

import argparse
import random
import json
from pathlib import Path
from datasets import Dataset
import numpy as np

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # ç®€å•çš„è¿›åº¦æ˜¾ç¤ºæ›¿ä»£
    def tqdm(iterable, **kwargs):
        return iterable


# ä½¿ç”¨ lerobot çš„æ ‡å‡†ç¼“å­˜ç›®å½•
try:
    from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset
    DEFAULT_CACHE_DIR = HF_LEROBOT_HOME
    HAS_LEROBOT = True
except ImportError:
    # å¦‚æœæ²¡æœ‰å®‰è£… lerobotï¼Œä½¿ç”¨ HuggingFace çš„é»˜è®¤ç¼“å­˜
    from huggingface_hub import constants
    DEFAULT_CACHE_DIR = Path(constants.HF_HOME) / "lerobot"
    HAS_LEROBOT = False
    print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° lerobot åº“ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

print(f"ğŸ“ ä½¿ç”¨ç¼“å­˜ç›®å½•: {DEFAULT_CACHE_DIR}")


def create_subset_repo_id(
    base_repo_id: str,
    fraction: float,
    seed: int,
    split: str,
    suffix: str = None
) -> str:
    """
    ç”Ÿæˆå­æ•°æ®é›†çš„ repo_id
    
    ä¾‹å¦‚ï¼š
    base: "Coil1987121/calvin_lerobot_task_ABCD_D_training"
    è¾“å‡º: "Coil1987121/calvin_5percent_seed42_training"
    """
    # è§£æ base_repo_id
    if "/" in base_repo_id:
        namespace, dataset_name = base_repo_id.split("/", 1)
    else:
        namespace = ""
        dataset_name = base_repo_id
    
    # ç§»é™¤ split åç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for s in ["_training", "_train", "_validation", "_val"]:
        if dataset_name.endswith(s):
            dataset_name = dataset_name[:-len(s)]
            break
    
    # ç”Ÿæˆæ–°åç§°
    if suffix:
        subset_name = f"{dataset_name}_{suffix}"
    else:
        fraction_str = str(int(fraction * 100)) + "percent"
        subset_name = f"{dataset_name}_{fraction_str}_seed{seed}"
    
    # æ·»åŠ  split åç¼€
    if split == "train":
        subset_name += "_training"
    elif split == "validation":
        subset_name += "_validation"
    
    if namespace:
        return f"{namespace}/{subset_name}"
    return subset_name


def get_cache_path(repo_id: str) -> Path:
    """è·å–æ•°æ®é›†åœ¨ç¼“å­˜ä¸­çš„è·¯å¾„"""
    return DEFAULT_CACHE_DIR / repo_id


def dataset_exists_in_cache(repo_id: str) -> bool:
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨äºç¼“å­˜ä¸­"""
    cache_path = get_cache_path(repo_id)
    return cache_path.exists() and (cache_path / "data").exists()


def get_cached_dataset_info(repo_id: str) -> dict:
    """è·å–ç¼“å­˜ä¸­æ•°æ®é›†çš„ä¿¡æ¯"""
    cache_path = get_cache_path(repo_id)
    
    if not dataset_exists_in_cache(repo_id):
        return None
    
    info = {
        "cache_path": str(cache_path),
        "exists": True,
    }
    
    # å°è¯•ä½¿ç”¨ LeRobotDataset è¯»å–ä¿¡æ¯ï¼ˆæœ€å‡†ç¡®ï¼‰
    if HAS_LEROBOT:
        try:
            import os
            old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
            os.environ['HF_HUB_OFFLINE'] = '1'
            
            try:
                dataset = LeRobotDataset(repo_id, root=DEFAULT_CACHE_DIR, local_files_only=True)
                info['total_episodes'] = dataset.num_episodes
                info['total_frames'] = len(dataset)
                info['fps'] = dataset.fps
                info['robot_type'] = dataset.robot_type
                return info
            finally:
                if old_hf_hub_offline is None:
                    os.environ.pop('HF_HUB_OFFLINE', None)
                else:
                    os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
        except Exception:
            pass
    
    # å°è¯•è¯»å–å…ƒæ•°æ®æ–‡ä»¶
    metadata_path = cache_path / "meta" / "info.json"
    if metadata_path.exists():
        try:
            with open(metadata_path, 'r') as f:
                meta = json.load(f)
                info.update(meta)
        except Exception:
            pass
    
    return info


def create_subset(
    source_repo_id: str,
    target_repo_id: str,
    fraction: float = 0.05,
    seed: int = 42,
    split: str = "train",
):
    """
    åˆ›å»ºæ•°æ®é›†å­é›†å¹¶ä¿å­˜åˆ° HuggingFace ç¼“å­˜
    
    Args:
        source_repo_id: æºæ•°æ®é›†çš„ repo ID
        target_repo_id: ç›®æ ‡å­é›†çš„ repo ID
        fraction: é‡‡æ ·æ¯”ä¾‹
        seed: éšæœºç§å­
        split: æ•°æ®é›†åˆ†å‰²
    """
    
    print("=" * 70)
    print(f"ğŸš€ åˆ›å»ºæ•°æ®é›†å­é›†")
    print("=" * 70)
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   æºæ•°æ®é›†: {source_repo_id}")
    print(f"   ç›®æ ‡å­é›†: {target_repo_id}")
    print(f"   åˆ†å‰²: {split}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {fraction * 100:.1f}%")
    print(f"   éšæœºç§å­: {seed}")
    
    target_cache_path = get_cache_path(target_repo_id)
    print(f"   ç¼“å­˜è·¯å¾„: {target_cache_path}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if dataset_exists_in_cache(target_repo_id):
        print(f"\nâš ï¸  ç›®æ ‡æ•°æ®é›†å·²å­˜åœ¨äºç¼“å­˜ä¸­")
        info = get_cached_dataset_info(target_repo_id)
        if info:
            print(f"   Episodes: {info.get('total_episodes', 'unknown')}")
            print(f"   Frames: {info.get('total_frames', 'unknown')}")
        
        response = input("   æ˜¯å¦è¦†ç›–? [y/N]: ").strip().lower()
        if response != 'y':
            print("   å–æ¶ˆæ“ä½œ")
            return None
        
        print(f"   åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®é›†...")
        import shutil
        shutil.rmtree(target_cache_path)
    
    # 1. åŠ è½½æºæ•°æ®é›†
    print(f"\nğŸ“¥ æ­¥éª¤ 1/4: åŠ è½½æºæ•°æ®é›†...")
    print(f"   {source_repo_id}")
    
    if not HAS_LEROBOT:
        print(f"   âŒ éœ€è¦å®‰è£… lerobot åº“æ‰èƒ½åŠ è½½æ•°æ®é›†")
        print(f"   pip install lerobot")
        return None
    
    try:
        # ä½¿ç”¨ LeRobotDataset åŠ è½½
        source_cache_path = get_cache_path(source_repo_id)
        
        if not source_cache_path.exists():
            print(f"   âŒ æºæ•°æ®é›†ä¸å­˜åœ¨: {source_cache_path}")
            print(f"\nğŸ’¡ æç¤º:")
            print(f"   1. ç¡®ä¿å·²è½¬æ¢æ•°æ®é›†åˆ° LeRobot æ ¼å¼")
            print(f"   2. è¿è¡Œ 'uv run create_subset.py --list' æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
            print(f"   3. æ•°æ®é›†è·¯å¾„åº”è¯¥æ˜¯: {DEFAULT_CACHE_DIR}/{source_repo_id}")
            return None
        
        print(f"   ä»ç¼“å­˜åŠ è½½: {source_cache_path}")
        print(f"   â³ åŠ è½½ä¸­...", end=" ", flush=True)
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è®¿é—® HuggingFace Hub
        import os
        old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        try:
            # ä½¿ç”¨ local_files_only å‚æ•°å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            # root åº”è¯¥æ˜¯ repo_id çš„çˆ¶ç›®å½•
            lerobot_dataset = LeRobotDataset(
                source_repo_id,
                root=DEFAULT_CACHE_DIR,
                local_files_only=True,
            )
        finally:
            # æ¢å¤åŸæ¥çš„ç¯å¢ƒå˜é‡
            if old_hf_hub_offline is None:
                os.environ.pop('HF_HUB_OFFLINE', None)
            else:
                os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
        
        print(f"âœ…")
        print(f"   ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"      Episodes: {lerobot_dataset.num_episodes}")
        print(f"      Frames: {len(lerobot_dataset)}")
        print(f"      FPS: {lerobot_dataset.fps}")
        print(f"      Robot: {lerobot_dataset.robot_type}")
        
        # è½¬æ¢ä¸º HuggingFace Dataset æ ¼å¼ä»¥ä¾¿å¤„ç†
        print(f"   ğŸ“¦ å‡†å¤‡æ•°æ®...", end=" ", flush=True)
        dataset = lerobot_dataset.hf_dataset
        print(f"âœ…")
        
    except Exception as e:
        print(f"\n   âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print(f"\nğŸ’¡ æç¤º:")
        print(f"   1. ç¡®ä¿æ•°æ®é›†å·²ç»è½¬æ¢ä¸º LeRobot æ ¼å¼")
        print(f"   2. æ£€æŸ¥æ•°æ®é›†è·¯å¾„: {get_cache_path(source_repo_id)}")
        print(f"   3. æ£€æŸ¥ meta/info.json æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print(f"   4. å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè„šæœ¬å·²å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
        return None
    
    # 2. é‡‡æ · episodes
    print(f"\nğŸ“Š æ­¥éª¤ 2/4: é‡‡æ · episodes...")
    
    all_episodes = sorted(set(dataset['episode_index']))
    total_episodes = len(all_episodes)
    num_sampled = max(1, int(total_episodes * fraction))
    
    print(f"   æ€» episodes: {total_episodes}")
    print(f"   é‡‡æ ·æ•°é‡: {num_sampled} ({fraction*100:.1f}%)")
    print(f"   éšæœºç§å­: {seed}")
    
    random.seed(seed)
    sampled_episodes = set(random.sample(all_episodes, num_sampled))
    
    print(f"   é‡‡æ ·çš„ episodes (å‰10ä¸ª): {sorted(list(sampled_episodes))[:10]}")
    
    # 3. è¿‡æ»¤æ•°æ®é›†
    print(f"\nğŸ” æ­¥éª¤ 3/4: è¿‡æ»¤å’Œé‡ç»„æ•°æ®é›†...")
    
    original_size = len(dataset)
    print(f"   åŸå§‹æ ·æœ¬æ•°: {original_size}")
    print(f"   â³ è¿‡æ»¤ä¸­...", end=" ", flush=True)
    
    # æŒ‰ episode è¿‡æ»¤
    sampled_dataset = dataset.filter(
        lambda x: x['episode_index'] in sampled_episodes,
        desc="è¿‡æ»¤ episodes"
    )
    new_size = len(sampled_dataset)
    actual_fraction = new_size / original_size if original_size > 0 else 0
    
    print(f"âœ…")
    print(f"   é‡‡æ ·æ ·æœ¬æ•°: {new_size}")
    print(f"   å®é™…æ¯”ä¾‹: {actual_fraction * 100:.2f}%")
    
    # 4. ä¿å­˜ä¸ºæ–°çš„ LeRobot æ•°æ®é›†
    print(f"\nğŸ’¾ æ­¥éª¤ 4/4: ä¿å­˜ä¸º LeRobot æ•°æ®é›†...")
    print(f"   ç›®æ ‡è·¯å¾„: {target_cache_path}")
    
    try:
        # åˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
        if target_cache_path.exists():
            import shutil
            shutil.rmtree(target_cache_path)
        
        target_cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è·å–æºæ•°æ®é›†çš„ features å’Œå…ƒæ•°æ®
        source_info = lerobot_dataset.info
        
        print(f"   ğŸ“‹ æ•°æ®é›†é…ç½®:")
        print(f"      Robot: {source_info.get('robot_type', 'unknown')}")
        print(f"      FPS: {source_info.get('fps', 30)}")
        
        print(f"   â³ åˆ›å»ºæ–°æ•°æ®é›†...", end=" ", flush=True)
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼
        import os
        old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        try:
            # åˆ›å»ºæ–°çš„ LeRobot æ•°æ®é›†
            new_dataset = LeRobotDataset.create(
                repo_id=target_repo_id,
                fps=source_info.get('fps', 30),
                robot_type=source_info.get('robot_type', 'franka'),
                features=lerobot_dataset.features,
                image_writer_threads=4,
                image_writer_processes=2,
                local_files_only=True,
            )
        finally:
            # æ¢å¤ç¯å¢ƒå˜é‡
            if old_hf_hub_offline is None:
                os.environ.pop('HF_HUB_OFFLINE', None)
            else:
                os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
        
        print(f"âœ…")
        print(f"   ğŸ“¦ æ·»åŠ æ•°æ® (å…± {num_sampled} episodes)...")
        
        # æŒ‰ episode ç»„ç»‡æ•°æ®
        episode_data = {}
        for idx in range(len(sampled_dataset)):
            sample = sampled_dataset[idx]
            ep_idx = sample['episode_index']
            
            if ep_idx not in episode_data:
                episode_data[ep_idx] = []
            
            episode_data[ep_idx].append(sample)
        
        # é€ episode æ·»åŠ ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
        for ep_idx, frames in tqdm(
            sorted(episode_data.items()), 
            desc="   ä¿å­˜ episodes",
            unit="ep",
            disable=not HAS_TQDM
        ):
            for frame in frames:
                # å‡†å¤‡å¸§æ•°æ®
                frame_dict = {}
                for key in frame.keys():
                    if key not in ['index', 'episode_index', 'frame_index', 'timestamp']:
                        frame_dict[key] = frame[key]
                
                new_dataset.add_frame(frame_dict)
            
            new_dataset.save_episode()
        
        print(f"\n   âœ… ä¿å­˜æˆåŠŸ")
        print(f"   ğŸ“Š æ–°æ•°æ®é›†ç»Ÿè®¡:")
        print(f"      Episodes: {new_dataset.num_episodes}")
        print(f"      Frames: {len(new_dataset)}")
        
    except Exception as e:
        print(f"\n   âŒ ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # 5. åˆ›å»ºå…ƒæ•°æ®
    metadata = {
        'source_repo_id': source_repo_id,
        'target_repo_id': target_repo_id,
        'split': split,
        'fraction': fraction,
        'seed': seed,
        'total_episodes': len(sampled_episodes),
        'total_frames': len(new_dataset),
        'original_episodes': total_episodes,
        'original_frames': original_size,
        'actual_fraction': actual_fraction,
        'sampled_episode_ids': sorted(list(sampled_episodes)),
    }
    
    metadata_path = target_cache_path / "subset_metadata.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nğŸ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… å­æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   Episodes: {len(sampled_episodes)} / {total_episodes} ({actual_fraction*100:.2f}%)")
    print(f"   Frames: {len(new_dataset)} / {original_size}")
    print(f"\nğŸ“ ä½ç½®:")
    print(f"   {target_cache_path}")
    print(f"\nğŸ’¡ åœ¨è®­ç»ƒé…ç½®ä¸­ä½¿ç”¨:")
    print(f"   repo_id=\"{target_repo_id}\"")
    
    return str(target_cache_path)


def list_cached_datasets():
    """åˆ—å‡ºç¼“å­˜ä¸­çš„æ‰€æœ‰æ•°æ®é›†"""
    print("\nğŸ“‚ HuggingFace ç¼“å­˜ä¸­çš„æ•°æ®é›†:")
    print("=" * 70)
    
    if not DEFAULT_CACHE_DIR.exists():
        print(f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {DEFAULT_CACHE_DIR}")
        print("\nğŸ’¡ æç¤º: è¯·å…ˆè½¬æ¢æ•°æ®é›†åˆ° LeRobot æ ¼å¼")
        return
    
    print(f"ğŸ“ æ‰«æç›®å½•: {DEFAULT_CACHE_DIR}")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®é›†
    datasets = []
    
    # æ‰«æç¼“å­˜ç›®å½•
    try:
        namespace_dirs = [d for d in DEFAULT_CACHE_DIR.iterdir() if d.is_dir()]
        print(f"ğŸ“‚ æ‰¾åˆ° {len(namespace_dirs)} ä¸ªå‘½åç©ºé—´ç›®å½•")
        
        for namespace_dir in namespace_dirs:
            print(f"\nğŸ” æ‰«æå‘½åç©ºé—´: {namespace_dir.name}")
            
            try:
                dataset_dirs = [d for d in namespace_dir.iterdir() if d.is_dir()]
                print(f"   æ‰¾åˆ° {len(dataset_dirs)} ä¸ªç›®å½•")
                
                for dataset_dir in dataset_dirs:
                    print(f"   ğŸ” æ£€æŸ¥: {dataset_dir.name}", end=" ... ")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ LeRobot æ•°æ®é›†ï¼ˆæœ‰ data å’Œ meta ç›®å½•ï¼‰
                    has_data = (dataset_dir / "data").exists()
                    has_meta = (dataset_dir / "meta").exists()
                    
                    if has_data and has_meta:
                        print("âœ…")
                        repo_id = f"{namespace_dir.name}/{dataset_dir.name}"
                        
                        # å°è¯•è·å–ä¿¡æ¯
                        print(f"      ğŸ“Š åŠ è½½ä¿¡æ¯...", end=" ")
                        try:
                            info = get_cached_dataset_info(repo_id)
                            if info:
                                print("âœ…")
                                datasets.append({
                                    'repo_id': repo_id,
                                    'episodes': info.get('total_episodes', '?'),
                                    'frames': info.get('total_frames', '?'),
                                    'fps': info.get('fps', '?'),
                                    'robot': info.get('robot_type', '?'),
                                    'path': str(dataset_dir)
                                })
                            else:
                                print("âš ï¸  (æ— æ³•è¯»å–ä¿¡æ¯)")
                        except Exception as e:
                            print(f"âŒ ({e})")
                    else:
                        print(f"â­ï¸  (ä¸æ˜¯ LeRobot æ•°æ®é›†: data={has_data}, meta={has_meta})")
                        
            except Exception as e:
                print(f"   âŒ æ‰«æå¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ æ‰«æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "=" * 70)
    
    if not datasets:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†")
        print("\nğŸ’¡ æç¤º:")
        print("   1. ç¡®ä¿å·²è¿è¡Œæ•°æ®è½¬æ¢è„šæœ¬")
        print("   2. æ£€æŸ¥æ•°æ®é›†ç›®å½•ç»“æ„ï¼ˆéœ€è¦ data/ å’Œ meta/ å­ç›®å½•ï¼‰")
        print(f"   3. å½“å‰ç¼“å­˜ç›®å½•: {DEFAULT_CACHE_DIR}")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†:\n")
    
    # æŒ‰åç§°æ’åº
    datasets.sort(key=lambda x: x['repo_id'])
    
    # æ‰“å°
    for ds in datasets:
        print(f"ğŸ“¦ {ds['repo_id']}")
        print(f"   Episodes: {ds['episodes']}")
        print(f"   Frames: {ds['frames']}")
        print(f"   FPS: {ds['fps']}")
        print(f"   Robot: {ds['robot']}")
        print(f"   Path: {ds['path']}")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="åˆ›å»ºæ•°æ®é›†å­é›†å¹¶ä¿å­˜åˆ° HuggingFace ç¼“å­˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--source_train_repo_id",
        type=str,
        default="Coil1987121/calvin_lerobot_task_ABCD_D_training",
        help="æºè®­ç»ƒé›†çš„ repo ID"
    )
    
    parser.add_argument(
        "--source_val_repo_id",
        type=str,
        default="Coil1987121/calvin_lerobot_task_ABCD_D_validation",
        help="æºéªŒè¯é›†çš„ repo ID"
    )
    
    parser.add_argument(
        "--fraction",
        type=float,
        default=0.05,
        help="é‡‡æ ·æ¯”ä¾‹ï¼Œé»˜è®¤ 0.05 (5%%)"
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­"
    )
    
    parser.add_argument(
        "--subset_suffix",
        type=str,
        default=None,
        help="å­é›†åç¼€åï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œè‡ªåŠ¨ç”Ÿæˆå¦‚ 5percent_seed42ï¼‰"
    )
    
    parser.add_argument(
        "--create_both",
        action="store_true",
        help="åŒæ—¶åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†"
    )
    
    parser.add_argument(
        "--split",
        type=str,
        default="train",
        choices=["train", "validation"],
        help="è¦åˆ›å»ºçš„åˆ†å‰²ï¼ˆå¦‚æœä¸ä½¿ç”¨ --create_bothï¼‰"
    )
    
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºç¼“å­˜ä¸­çš„æ‰€æœ‰æ•°æ®é›†"
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ•°æ®é›†
    if args.list:
        list_cached_datasets()
        return
    
    # éªŒè¯å‚æ•°
    if not 0 < args.fraction <= 1.0:
        print(f"âŒ é”™è¯¯: fraction å¿…é¡»åœ¨ (0, 1] èŒƒå›´å†…")
        return
    
    created_repos = []
    
    # åˆ›å»ºè®­ç»ƒé›†
    if args.create_both or args.split == "train":
        print("\nğŸ”µ åˆ›å»ºè®­ç»ƒé›†å­é›†...")
        
        target_train_repo_id = create_subset_repo_id(
            args.source_train_repo_id,
            args.fraction,
            args.seed,
            "train",
            args.subset_suffix
        )
        
        train_path = create_subset(
            source_repo_id=args.source_train_repo_id,
            target_repo_id=target_train_repo_id,
            fraction=args.fraction,
            seed=args.seed,
            split="train"
        )
        
        if train_path:
            created_repos.append(("è®­ç»ƒé›†", target_train_repo_id))
        print()
    
    # åˆ›å»ºéªŒè¯é›†
    if args.create_both or args.split == "validation":
        print("\nğŸŸ¢ åˆ›å»ºéªŒè¯é›†å­é›†...")
        
        target_val_repo_id = create_subset_repo_id(
            args.source_val_repo_id,
            args.fraction,
            args.seed,
            "validation",
            args.subset_suffix
        )
        
        val_path = create_subset(
            source_repo_id=args.source_val_repo_id,
            target_repo_id=target_val_repo_id,
            fraction=args.fraction,
            seed=args.seed,
            split="validation"
        )
        
        if val_path:
            created_repos.append(("éªŒè¯é›†", target_val_repo_id))
        print()
    
    # æœ€ç»ˆæ€»ç»“
    if created_repos:
        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰å­æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
        print("=" * 70)
        
        print("\nğŸ“¦ åˆ›å»ºçš„æ•°æ®é›†:")
        for name, repo_id in created_repos:
            cache_path = get_cache_path(repo_id)
            print(f"\n{name}:")
            print(f"  Repo ID: {repo_id}")
            print(f"  è·¯å¾„: {cache_path}")
        
        print("\nğŸ“ åœ¨è®­ç»ƒé…ç½®ä¸­ä½¿ç”¨:")
        print("```python")
        if len(created_repos) >= 1:
            train_repo = created_repos[0][1]
            print(f"data=LeRobotCALVINDataConfig(")
            print(f"    repo_id=\"{train_repo}\",")
            print(f"    # ... å…¶ä»–é…ç½®")
            print(f"),")
        if len(created_repos) >= 2:
            val_repo = created_repos[1][1]
            print(f"val_repo_id=\"{val_repo}\",")
        print("```")
        
        print("\nğŸ’¡ æç¤º:")
        print(f"  - æ‰€æœ‰æ•°æ®é›†éƒ½ä¿å­˜åœ¨: {DEFAULT_CACHE_DIR}")
        print(f"  - ä½¿ç”¨ --list æŸ¥çœ‹ç¼“å­˜ä¸­çš„æ‰€æœ‰æ•°æ®é›†")
        print(f"  - æ•°æ®é›†ä¼šè‡ªåŠ¨è¢« LeRobotDataset è¯†åˆ«")
    
    print()


if __name__ == "__main__":
    main()