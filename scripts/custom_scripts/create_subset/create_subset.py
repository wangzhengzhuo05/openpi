"""
Convert CALVIN dataset to LeRobot format - MEMORY OPTIMIZED VERSION WITH SAFETY & RESUME

ä¸»è¦æ”¹è¿›:
1. å‡å°‘å›¾åƒç¼“å­˜ï¼ŒåŠæ—¶é‡Šæ”¾å†…å­˜
2. é™ä½Žå¹¶å‘çº¿ç¨‹æ•°
3. æ·»åŠ åžƒåœ¾å›žæ”¶
4. åˆ†æ‰¹å¤„ç†episodes
5. å¯é€‰çš„å›¾åƒåŽ‹ç¼©
6. æ”¯æŒéšæœºé‡‡æ ·æŒ‡å®šæ¯”ä¾‹çš„æ•°æ®
7. ðŸ†• å®‰å…¨é˜²æŠ¤ï¼šåˆ é™¤å‰ç¡®è®¤ï¼Œæ”¯æŒå¤‡ä»½
8. ðŸ†• æ¢å¤åŠŸèƒ½ï¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œåˆ†æ‰¹å¤„ç†

Usage:
    # è½¬æ¢æ‰€æœ‰æ•°æ®
    python convert_calvin_to_lerobot_safe.py --data_dir /path/to/calvin
    
    # éšæœºé‡‡æ ·50%çš„æ•°æ®
    python convert_calvin_to_lerobot_safe.py --data_dir /path/to/calvin --sample_ratio 0.5
    
    # åˆ†æ‰¹å¤„ç†ï¼šå…ˆå¤„ç†10000ä¸ª
    python convert_calvin_to_lerobot_safe.py --data_dir /path/to/calvin --max_episodes 10000
    
    # ç»§ç»­å¤„ç†å‰©ä½™çš„ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤ï¼‰
    python convert_calvin_to_lerobot_safe.py --data_dir /path/to/calvin --resume
    
    # ä¸åˆ é™¤çŽ°æœ‰æ•°æ®ï¼Œç›´æŽ¥è¿½åŠ 
    python convert_calvin_to_lerobot_safe.py --data_dir /path/to/calvin --no_delete --resume
"""

import gc
import json
import os
import random
import shutil
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
from PIL import Image
import tyro
from datetime import datetime

from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset


class ProgressTracker:
    """è·Ÿè¸ªè½¬æ¢è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
    
    def __init__(self, checkpoint_file: Path):
        self.checkpoint_file = checkpoint_file
        self.processed_episodes: List[str] = []
        self.total_frames = 0
        self.start_time = None
        self.load()
    
    def load(self):
        """åŠ è½½å·²æœ‰çš„è¿›åº¦"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                data = json.load(f)
                self.processed_episodes = data.get('processed_episodes', [])
                self.total_frames = data.get('total_frames', 0)
                self.start_time = data.get('start_time')
                print(f"ðŸ“‚ Found existing progress: {len(self.processed_episodes)} episodes completed")
    
    def save(self):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        data = {
            'processed_episodes': self.processed_episodes,
            'total_frames': self.total_frames,
            'start_time': self.start_time,
            'last_update': datetime.now().isoformat()
        }
        self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.checkpoint_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def add_episode(self, episode_id: str, num_frames: int):
        """è®°å½•å·²å¤„ç†çš„episode"""
        if episode_id not in self.processed_episodes:
            self.processed_episodes.append(episode_id)
            self.total_frames += num_frames
            if self.start_time is None:
                self.start_time = datetime.now().isoformat()
            self.save()
    
    def is_processed(self, episode_id: str) -> bool:
        """æ£€æŸ¥episodeæ˜¯å¦å·²å¤„ç†"""
        return episode_id in self.processed_episodes
    
    def clear(self):
        """æ¸…é™¤è¿›åº¦è®°å½•"""
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()
        self.processed_episodes = []
        self.total_frames = 0
        self.start_time = None


def confirm_action(prompt: str, default: bool = False) -> bool:
    """
    è¯¢é—®ç”¨æˆ·ç¡®è®¤æ“ä½œ
    
    Args:
        prompt: æç¤ºä¿¡æ¯
        default: é»˜è®¤é€‰æ‹©
    
    Returns:
        ç”¨æˆ·æ˜¯å¦ç¡®è®¤
    """
    default_str = "[Y/n]" if default else "[y/N]"
    while True:
        response = input(f"{prompt} {default_str}: ").strip().lower()
        if response == '':
            return default
        if response in ['y', 'yes']:
            return True
        if response in ['n', 'no']:
            return False
        print("Please answer 'y' or 'n'")


def backup_dataset(output_path: Path) -> Path:
    """
    å¤‡ä»½çŽ°æœ‰æ•°æ®é›†
    
    Args:
        output_path: æ•°æ®é›†è·¯å¾„
    
    Returns:
        å¤‡ä»½è·¯å¾„
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = output_path.parent / f"{output_path.name}_backup_{timestamp}"
    
    print(f"ðŸ“¦ Creating backup: {backup_path}")
    shutil.copytree(output_path, backup_path)
    print(f"âœ“ Backup created successfully")
    
    return backup_path


def load_episode_json(json_path: Path) -> Dict[str, Any]:
    """Load episode JSON file."""
    with open(json_path, 'r') as f:
        return json.load(f)


def load_and_preprocess_image(
    image_path: Path, 
    target_size: tuple = (224, 224),
    quality: int = 95
) -> np.ndarray:
    """Load and resize image with memory optimization."""
    img = Image.open(image_path)
    
    if img.size[0] > target_size[1] * 2 or img.size[1] > target_size[0] * 2:
        img.thumbnail((target_size[1] * 2, target_size[0] * 2), Image.LANCZOS)
    
    img = img.resize((target_size[1], target_size[0]), Image.BILINEAR)
    img_array = np.array(img, dtype=np.uint8)
    
    img.close()
    del img
    
    return img_array


def get_frame_number(filename: str) -> int:
    """Extract frame number from filename."""
    return int(filename.split('_')[1].split('.')[0])


def pad_or_truncate_state(state: np.ndarray, target_dim: int = 32) -> np.ndarray:
    """Pad or truncate state vector to target dimension."""
    current_dim = len(state)
    
    if current_dim == target_dim:
        return state
    elif current_dim < target_dim:
        padding = np.zeros(target_dim - current_dim, dtype=np.float32)
        return np.concatenate([state, padding])
    else:
        return state[:target_dim]


def select_episodes(
    json_files: list,
    start_episode: int = 0,
    max_episodes: int | None = None,
    sample_ratio: float | None = None,
    sample_count: int | None = None,
    random_seed: int | None = None,
) -> list:
    """Select episodes based on various criteria."""
    if sample_ratio is not None or sample_count is not None:
        if random_seed is not None:
            random.seed(random_seed)
            np.random.seed(random_seed)
        
        if sample_count is not None:
            num_samples = min(sample_count, len(json_files))
        elif sample_ratio is not None:
            if not 0.0 < sample_ratio <= 1.0:
                raise ValueError(f"sample_ratio must be between 0.0 and 1.0, got {sample_ratio}")
            num_samples = max(1, int(len(json_files) * sample_ratio))
        
        print(f"ðŸŽ² Random sampling {num_samples} episodes from {len(json_files)} total episodes")
        if random_seed is not None:
            print(f"   Random seed: {random_seed}")
        
        selected_files = random.sample(json_files, num_samples)
        selected_files = sorted(selected_files)
        
        return selected_files
    else:
        selected_files = json_files[start_episode:]
        if max_episodes:
            selected_files = selected_files[:max_episodes]
        return selected_files


def handle_existing_dataset(
    output_path: Path,
    no_delete: bool,
    force_delete: bool,
    create_backup: bool,
    resume: bool
) -> bool:
    """
    å¤„ç†å·²å­˜åœ¨çš„æ•°æ®é›†
    
    Args:
        output_path: æ•°æ®é›†è·¯å¾„
        no_delete: æ˜¯å¦ç¦æ­¢åˆ é™¤
        force_delete: æ˜¯å¦å¼ºåˆ¶åˆ é™¤ï¼ˆä¸è¯¢é—®ï¼‰
        create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
        resume: æ˜¯å¦æ¢å¤æ¨¡å¼
    
    Returns:
        æ˜¯å¦åº”è¯¥ç»§ç»­ï¼ˆTrueï¼‰æˆ–é€€å‡ºï¼ˆFalseï¼‰
    """
    if not output_path.exists():
        return True
    
    print(f"\nâš ï¸  Found existing dataset at: {output_path}")
    
    # å¦‚æžœæ˜¯æ¢å¤æ¨¡å¼ï¼Œä¸åˆ é™¤
    if resume:
        print("ðŸ“¥ Resume mode: Will continue from existing dataset")
        return True
    
    # å¦‚æžœè®¾ç½®äº†ä¸åˆ é™¤
    if no_delete:
        print("ðŸ”’ No-delete mode: Will append to existing dataset")
        return True
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    try:
        # å°è¯•ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        total_size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file())
        total_size_mb = total_size / (1024 * 1024)
        print(f"   Size: {total_size_mb:.2f} MB")
    except Exception as e:
        print(f"   (Unable to get size info: {e})")
    
    # å¼ºåˆ¶åˆ é™¤æ¨¡å¼
    if force_delete:
        if create_backup:
            backup_dataset(output_path)
        print(f"ðŸ—‘ï¸  Force delete mode: Removing existing dataset")
        shutil.rmtree(output_path)
        return True
    
    # è¯¢é—®ç”¨æˆ·
    print("\nOptions:")
    print("  1. Delete existing dataset and start fresh")
    print("  2. Create backup before deleting")
    print("  3. Append to existing dataset (resume)")
    print("  4. Cancel and exit")
    
    while True:
        choice = input("\nYour choice [1/2/3/4]: ").strip()
        
        if choice == '1':
            if confirm_action("âš ï¸  Are you sure you want to DELETE the existing dataset?", default=False):
                print(f"ðŸ—‘ï¸  Removing existing dataset...")
                shutil.rmtree(output_path)
                return True
            else:
                print("Operation cancelled")
                return False
        
        elif choice == '2':
            backup_dataset(output_path)
            print(f"ðŸ—‘ï¸  Removing existing dataset...")
            shutil.rmtree(output_path)
            return True
        
        elif choice == '3':
            print("ðŸ“¥ Continuing with existing dataset")
            return True
        
        elif choice == '4':
            print("âŒ Operation cancelled by user")
            return False
        
        else:
            print("Invalid choice. Please enter 1, 2, 3, or 4")


def main(
    data_dir: str = "/root/autodl-tmp/task_ABCD_D_processed/training",
    *,
    repo_name: str = "Coil1987121/calvin_lerobot_task_ABCD_D_training",
    push_to_hub: bool = False,
    include_depth: bool = False,
    include_tactile: bool = False,
    max_episodes: int | None = None,
    start_episode: int = 0,
    state_dim: int = 32,
    fps: int = 30,
    # å†…å­˜ä¼˜åŒ–å‚æ•°
    image_quality: int = 95,
    batch_save_episodes: int = 10,
    writer_threads: int = 2,
    writer_processes: int = 1,
    # éšæœºé‡‡æ ·å‚æ•°
    sample_ratio: float | None = None,
    sample_count: int | None = None,
    random_seed: int | None = 42,
    # ðŸ†• å®‰å…¨åŠŸèƒ½å‚æ•°
    no_delete: bool = False,  # ç¦æ­¢åˆ é™¤çŽ°æœ‰æ•°æ®é›†
    force_delete: bool = False,  # å¼ºåˆ¶åˆ é™¤ï¼Œä¸è¯¢é—®ï¼ˆå±é™©ï¼ï¼‰
    create_backup: bool = False,  # åˆ é™¤å‰åˆ›å»ºå¤‡ä»½
    # ðŸ†• æ¢å¤åŠŸèƒ½å‚æ•°
    resume: bool = False,  # ä»Žä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­
    checkpoint_dir: str | None = None,  # æ£€æŸ¥ç‚¹æ–‡ä»¶ç›®å½•
):
    """
    Convert CALVIN dataset to LeRobot format with safety and resume features.
    
    å†…å­˜ä¼˜åŒ–å‚æ•°:
        image_quality: å›¾åƒè´¨é‡ (1-100)
        batch_save_episodes: æ¯å¤„ç†Nä¸ªepisodesåŽå¼ºåˆ¶åžƒåœ¾å›žæ”¶
        writer_threads: å›¾åƒå†™å…¥çº¿ç¨‹æ•°
    
    éšæœºé‡‡æ ·å‚æ•°:
        sample_ratio: éšæœºé‡‡æ ·æ¯”ä¾‹ (0.0-1.0)
        sample_count: éšæœºé‡‡æ ·æ•°é‡
        random_seed: éšæœºç§å­
    
    ðŸ†• å®‰å…¨åŠŸèƒ½å‚æ•°:
        no_delete: ç¦æ­¢åˆ é™¤çŽ°æœ‰æ•°æ®é›†ï¼Œåªè¿½åŠ ï¼ˆé»˜è®¤Falseï¼‰
        force_delete: å¼ºåˆ¶åˆ é™¤çŽ°æœ‰æ•°æ®é›†ï¼Œä¸è¯¢é—®ï¼ˆé»˜è®¤Falseï¼Œå±é™©ï¼ï¼‰
        create_backup: åˆ é™¤å‰åˆ›å»ºå¤‡ä»½ï¼ˆé»˜è®¤Falseï¼‰
    
    ðŸ†• æ¢å¤åŠŸèƒ½å‚æ•°:
        resume: ä»Žä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆé»˜è®¤Falseï¼‰
        checkpoint_dir: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨æ•°æ®é›†ç›®å½•ï¼‰
    
    ä½¿ç”¨åœºæ™¯:
        1. åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†:
           # ç¬¬ä¸€æ‰¹ï¼šå¤„ç†å‰10000ä¸ª
           python script.py --max_episodes 10000
           
           # ç¬¬äºŒæ‰¹ï¼šç»§ç»­å¤„ç†å‰©ä½™çš„
           python script.py --resume
        
        2. æ„å¤–ä¸­æ–­åŽæ¢å¤:
           python script.py --resume
        
        3. å®‰å…¨åˆ é™¤ï¼ˆå¸¦ç¡®è®¤ï¼‰:
           python script.py  # ä¼šè¯¢é—®æ˜¯å¦åˆ é™¤
        
        4. å¼ºåˆ¶åˆ é™¤ï¼ˆä¸è¯¢é—®ï¼Œå±é™©ï¼‰:
           python script.py --force_delete
        
        5. åˆ é™¤å‰å¤‡ä»½:
           python script.py --create_backup
    """
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise ValueError(f"Data directory does not exist: {data_dir}")
    
    # è®¾ç½®æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    output_path = HF_LEROBOT_HOME / repo_name
    if checkpoint_dir is None:
        checkpoint_file = output_path.parent / f".{repo_name.replace('/', '_')}_checkpoint.json"
    else:
        checkpoint_file = Path(checkpoint_dir) / f"{repo_name.replace('/', '_')}_checkpoint.json"
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
    progress = ProgressTracker(checkpoint_file)
    
    # å¤„ç†å·²å­˜åœ¨çš„æ•°æ®é›†ï¼ˆå®‰å…¨åŠŸèƒ½ï¼‰
    if not handle_existing_dataset(
        output_path=output_path,
        no_delete=no_delete,
        force_delete=force_delete,
        create_backup=create_backup,
        resume=resume
    ):
        print("\nâŒ Exiting...")
        return
    
    # å¦‚æžœä¸æ˜¯æ¢å¤æ¨¡å¼ä¸”åˆ é™¤äº†æ•°æ®é›†ï¼Œæ¸…é™¤è¿›åº¦
    if not resume and not no_delete and not output_path.exists():
        progress.clear()
        print("ðŸ”„ Progress cleared for fresh start")
    
    # Get episode files
    json_files = sorted(data_path.glob("lang_ann_*.json"))
    if not json_files:
        raise ValueError(f"No episode JSON files found in {data_dir}")
    
    print(f"\nðŸ“Š Dataset info:")
    print(f"   Total episodes available: {len(json_files)}")
    if progress.processed_episodes:
        print(f"   Already processed: {len(progress.processed_episodes)} episodes")
        print(f"   Remaining: {len(json_files) - len(progress.processed_episodes)} episodes")
    
    # Load first episode to determine dimensions
    first_episode = load_episode_json(json_files[0])
    print(f"\nðŸ¤– Robot state dimension: {state_dim}")
    
    # Check tactile dimensions if enabled
    tactile_rgb_dim = 0
    tactile_depth_dim = 0
    if include_tactile:
        images_dir = data_path / "images" / first_episode["episode_id"]
        tactile_rgb_dir = images_dir / "tactile_rgb"
        if tactile_rgb_dir.exists():
            sample_files = sorted(list(tactile_rgb_dir.glob("*.npy")))
            if sample_files:
                sample_data = np.load(sample_files[0])
                tactile_rgb_dim = sample_data.size
                print(f"   Tactile RGB dimension: {tactile_rgb_dim}")
                del sample_data
        
        tactile_depth_dir = images_dir / "tactile_depth"
        if tactile_depth_dir.exists():
            sample_files = sorted(list(tactile_depth_dir.glob("*.npy")))
            if sample_files:
                sample_data = np.load(sample_files[0])
                tactile_depth_dim = sample_data.size
                print(f"   Tactile depth dimension: {tactile_depth_dim}")
                del sample_data
    
    total_state_dim = state_dim + tactile_rgb_dim + tactile_depth_dim
    print(f"   Total state dimension: {total_state_dim}")
    
    # Define features
    features = {
        "observation.images.base_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.left_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.right_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.state": {
            "dtype": "float32",
            "shape": (state_dim,),
            "names": ["state"],
        },
        "actions": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["actions"],
        },
    }
    
    if include_depth:
        features["observation.images.depth_static"] = {
            "dtype": "image",
            "shape": (200, 200, 3),
            "names": ["height", "width", "channel"],
        }
        features["observation.images.depth_gripper"] = {
            "dtype": "image",
            "shape": (84, 84, 3),
            "names": ["height", "width", "channel"],
        }
    
    print("\nðŸ”§ Creating LeRobot dataset with features:")
    for key, value in features.items():
        print(f"   {key}: shape={value['shape']}, dtype={value['dtype']}")
    
    print(f"\nâš™ï¸  Memory optimization settings:")
    print(f"   Image quality: {image_quality}")
    print(f"   Writer threads: {writer_threads}")
    print(f"   GC batch size: {batch_save_episodes} episodes")
    
    # Create or load dataset
    if output_path.exists() and (resume or no_delete):
        print(f"\nðŸ“‚ Loading existing dataset from: {output_path}")
        dataset = LeRobotDataset(repo_id=repo_name)
        print(f"   Current dataset size: {len(dataset)} frames")
    else:
        print(f"\nðŸ†• Creating new dataset at: {output_path}")
        dataset = LeRobotDataset.create(
            repo_id=repo_name,
            robot_type="franka",
            fps=fps,
            features=features,
            image_writer_threads=writer_threads,
            image_writer_processes=writer_processes,
        )
    
    # Select episodes
    selected_files = select_episodes(
        json_files=json_files,
        start_episode=start_episode,
        max_episodes=max_episodes,
        sample_ratio=sample_ratio,
        sample_count=sample_count,
        random_seed=random_seed,
    )
    
    # Filter out already processed episodes (if resuming)
    if resume or no_delete:
        original_count = len(selected_files)
        selected_files = [f for f in selected_files if not progress.is_processed(load_episode_json(f)["episode_id"])]
        skipped = original_count - len(selected_files)
        if skipped > 0:
            print(f"\nâ­ï¸  Skipping {skipped} already processed episodes")
    
    if not selected_files:
        print(f"\nâœ… All episodes already processed! Nothing to do.")
        print(f"   Total frames in dataset: {len(dataset)}")
        return
    
    print(f"\nðŸš€ Processing {len(selected_files)} episodes...")
    if sample_ratio is not None or sample_count is not None:
        print(f"   Sampling strategy: ", end="")
        if sample_count is not None:
            print(f"Fixed count ({sample_count} episodes)")
        else:
            print(f"Ratio-based ({sample_ratio*100:.1f}% of total)")
    
    # Process episodes
    processed_count = 0
    for list_idx, json_path in enumerate(selected_files):
        
        # å®šæœŸå¼ºåˆ¶åžƒåœ¾å›žæ”¶
        if list_idx > 0 and list_idx % batch_save_episodes == 0:
            print(f"\nðŸ—‘ï¸  Running garbage collection after {list_idx} episodes...")
            gc.collect()
        
        episode_data = load_episode_json(json_path)
        episode_id = episode_data["episode_id"]
        
        # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆåŒé‡ä¿é™©ï¼‰
        if progress.is_processed(episode_id):
            continue
        
        # èŽ·å–åŽŸå§‹episodeç´¢å¼•
        original_idx = json_files.index(json_path)
        
        print(f"\nðŸ“ Episode {list_idx + 1}/{len(selected_files)} (original #{original_idx}): {episode_id}")
        print(f"   Task: {episode_data['language_instruction']}")
        print(f"   Frames: {len(episode_data['trajectory']['robot_observations'])}")
        
        images_dir = data_path / "images" / episode_id
        rgb_static_dir = images_dir / "rgb_static"
        frame_files = sorted(list(rgb_static_dir.glob("*.png")))
        frame_numbers = [get_frame_number(f.name) for f in frame_files]
        
        print(f"   Image frames: {len(frame_numbers)}")
        
        num_trajectory_frames = len(episode_data['trajectory']['robot_observations'])
        if len(frame_numbers) != num_trajectory_frames:
            print(f"   âš ï¸  Warning: Mismatch between trajectory ({num_trajectory_frames}) and images ({len(frame_numbers)})")
            num_frames = min(len(frame_numbers), num_trajectory_frames)
            frame_numbers = frame_numbers[:num_frames]
        else:
            num_frames = num_trajectory_frames
        
        # Process frames
        for frame_idx in range(num_frames):
            frame_num = frame_numbers[frame_idx]
            frame_name = f"frame_{frame_num:07d}"
            
            frame_data = {}
            
            try:
                # Load RGB images
                rgb_static_path = images_dir / "rgb_static" / f"{frame_name}.png"
                rgb_gripper_path = images_dir / "rgb_gripper" / f"{frame_name}.png"
                
                frame_data["observation.images.base_0_rgb"] = load_and_preprocess_image(
                    rgb_static_path, quality=image_quality
                )
                frame_data["observation.images.left_wrist_0_rgb"] = load_and_preprocess_image(
                    rgb_gripper_path, quality=image_quality
                )
                frame_data["observation.images.right_wrist_0_rgb"] = load_and_preprocess_image(
                    rgb_gripper_path, quality=image_quality
                )
                
                # Load depth images if needed
                if include_depth:
                    depth_static_path = images_dir / "depth_static" / f"{frame_name}.png"
                    depth_gripper_path = images_dir / "depth_gripper" / f"{frame_name}.png"
                    
                    depth_static = load_and_preprocess_image(depth_static_path, quality=image_quality)
                    depth_gripper = load_and_preprocess_image(depth_gripper_path, quality=image_quality)
                    
                    if len(depth_static.shape) == 2:
                        depth_static = np.stack([depth_static] * 3, axis=-1)
                    elif depth_static.shape[-1] == 1:
                        depth_static = np.repeat(depth_static, 3, axis=-1)
                    
                    if len(depth_gripper.shape) == 2:
                        depth_gripper = np.stack([depth_gripper] * 3, axis=-1)
                    elif depth_gripper.shape[-1] == 1:
                        depth_gripper = np.repeat(depth_gripper, 3, axis=-1)
                    
                    frame_data["observation.images.depth_static"] = depth_static
                    frame_data["observation.images.depth_gripper"] = depth_gripper
                
                # Build state vector
                robot_state = np.array(
                    episode_data["trajectory"]["robot_observations"][frame_idx],
                    dtype=np.float32
                )
                
                state_components = [robot_state]
                
                # Add tactile data if needed
                if include_tactile:
                    tactile_rgb_path = images_dir / "tactile_rgb" / f"{frame_name}.npy"
                    tactile_depth_path = images_dir / "tactile_depth" / f"{frame_name}.npy"
                    
                    if tactile_rgb_path.exists():
                        tactile_rgb = np.load(tactile_rgb_path).flatten().astype(np.float32)
                        state_components.append(tactile_rgb)
                    else:
                        state_components.append(np.zeros(tactile_rgb_dim, dtype=np.float32))
                    
                    if tactile_depth_path.exists():
                        tactile_depth = np.load(tactile_depth_path).flatten().astype(np.float32)
                        state_components.append(tactile_depth)
                    else:
                        state_components.append(np.zeros(tactile_depth_dim, dtype=np.float32))
                
                frame_data["observation.state"] = pad_or_truncate_state(np.concatenate(state_components))
                frame_data["actions"] = np.array(
                    episode_data["trajectory"]["actions"][frame_idx],
                    dtype=np.float32
                )
                frame_data["task"] = episode_data["language_instruction"]
                
                # Add frame
                dataset.add_frame(frame_data)
                
            finally:
                del frame_data
                
            # æ¯50å¸§åšä¸€æ¬¡è½»é‡çº§GC
            if frame_idx > 0 and frame_idx % 50 == 0:
                gc.collect(generation=0)
        
        # Save episode
        dataset.save_episode()
        
        # è®°å½•è¿›åº¦
        progress.add_episode(episode_id, num_frames)
        processed_count += 1
        
        print(f"   âœ“ Episode saved ({num_frames} frames) - Progress: {len(progress.processed_episodes)} total")
        
        # æ¸…ç†
        del episode_data
        del frame_numbers
        gc.collect(generation=0)
    
    # Final cleanup
    print(f"\nðŸ—‘ï¸  Final garbage collection...")
    gc.collect()
    
    # Summary
    print(f"\n{'='*60}")
    print(f"âœ… Conversion complete!")
    print(f"{'='*60}")
    print(f"Dataset location: {output_path}")
    print(f"Episodes processed this run: {processed_count}")
    print(f"Total episodes in dataset: {len(progress.processed_episodes)}")
    print(f"Total frames in dataset: {len(dataset)}")
    if len(json_files) > len(progress.processed_episodes):
        remaining = len(json_files) - len(progress.processed_episodes)
        print(f"\nðŸ“‹ Remaining episodes to process: {remaining}")
        print(f"   To continue, run with --resume flag")
    else:
        print(f"\nðŸŽ‰ All episodes processed!")
        print(f"   Checkpoint file can be safely deleted: {checkpoint_file}")
    
    if push_to_hub:
        print(f"\nðŸ“¤ Pushing dataset to Hugging Face Hub: {repo_name}")
        dataset.push_to_hub(
            tags=["calvin", "franka", "manipulation", "language-conditioned"],
            private=False,
            push_videos=True,
            license="mit",
        )
        print("âœ“ Dataset pushed to Hub!")


if __name__ == "__main__":
    tyro.cli(main)