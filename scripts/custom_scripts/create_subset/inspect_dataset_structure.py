#!/usr/bin/env python3
"""
æ•°æ®é›†ç»“æ„æ£€æŸ¥å·¥å…·
ç”¨äºè¯¦ç»†æŸ¥çœ‹ LeRobot æ•°æ®é›†çš„ç»“æ„ã€ç‰¹å¾å’Œæ•°æ®æ ¼å¼
"""

import argparse
import json
from pathlib import Path
import numpy as np

try:
    from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset
    DEFAULT_CACHE_DIR = HF_LEROBOT_HOME
    HAS_LEROBOT = True
except ImportError:
    from huggingface_hub import constants
    DEFAULT_CACHE_DIR = Path(constants.HF_HOME) / "lerobot"
    HAS_LEROBOT = False
    print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° lerobot åº“")


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def print_subsection(title):
    """æ‰“å°å­æ ‡é¢˜"""
    print(f"\n{'â”€' * 80}")
    print(f"  {title}")
    print(f"{'â”€' * 80}")


def inspect_dataset_structure(repo_id: str):
    """è¯¦ç»†æ£€æŸ¥æ•°æ®é›†ç»“æ„"""
    
    print_section(f"æ£€æŸ¥æ•°æ®é›†: {repo_id}")
    
    # 1. æ£€æŸ¥ç¼“å­˜è·¯å¾„
    cache_path = DEFAULT_CACHE_DIR / repo_id
    print(f"\nğŸ“ ç¼“å­˜è·¯å¾„: {cache_path}")
    print(f"   å­˜åœ¨: {cache_path.exists()}")
    
    if not cache_path.exists():
        print(f"\nâŒ æ•°æ®é›†ä¸å­˜åœ¨äºç¼“å­˜ä¸­")
        return
    
    # 2. æ£€æŸ¥æ–‡ä»¶ç»“æ„
    print_subsection("æ–‡ä»¶ç»“æ„")
    
    important_paths = [
        "meta/info.json",
        "meta/stats.json",
        "meta/episode_data_index.safetensors",
        "data",
    ]
    
    for rel_path in important_paths:
        full_path = cache_path / rel_path
        exists = full_path.exists()
        symbol = "âœ…" if exists else "âŒ"
        print(f"   {symbol} {rel_path}")
    
    # 3. è¯»å– meta/info.json
    print_subsection("æ•°æ®é›†å…ƒä¿¡æ¯ (meta/info.json)")
    
    info_path = cache_path / "meta" / "info.json"
    if info_path.exists():
        with open(info_path, 'r') as f:
            info = json.load(f)
        
        print(f"\n   åŸºæœ¬ä¿¡æ¯:")
        print(f"      æ•°æ®é›†åç§°: {info.get('codebase_version', 'N/A')}")
        print(f"      Robot ç±»å‹: {info.get('robot_type', 'N/A')}")
        print(f"      FPS: {info.get('fps', 'N/A')}")
        print(f"      æ€» Episodes: {info.get('total_episodes', 'N/A')}")
        print(f"      æ€» Frames: {info.get('total_frames', 'N/A')}")
        
        print(f"\n   ç‰¹å¾å®šä¹‰ (Features):")
        if 'features' in info:
            for key, feature_info in info['features'].items():
                print(f"\n      ğŸ“‹ {key}:")
                print(f"         ç±»å‹: {feature_info.get('dtype', 'N/A')}")
                if 'shape' in feature_info:
                    print(f"         å½¢çŠ¶: {feature_info['shape']}")
                if 'names' in feature_info:
                    print(f"         ç»´åº¦å: {feature_info['names']}")
                if 'info' in feature_info:
                    print(f"         ä¿¡æ¯: {feature_info['info']}")
    else:
        print("   âŒ æœªæ‰¾åˆ° meta/info.json")
    
    # 4. åŠ è½½æ•°æ®é›†å¹¶æ£€æŸ¥å®é™…æ•°æ®
    if not HAS_LEROBOT:
        print("\nâŒ éœ€è¦å®‰è£… lerobot åº“æ¥åŠ è½½æ•°æ®é›†")
        return
    
    print_subsection("åŠ è½½æ•°æ®é›†")
    
    try:
        import os
        old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        try:
            dataset = LeRobotDataset(repo_id, root=cache_path)
        finally:
            if old_hf_hub_offline is None:
                os.environ.pop('HF_HUB_OFFLINE', None)
            else:
                os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
        
        print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        print(f"   Episodes: {dataset.num_episodes}")
        print(f"   Frames: {len(dataset)}")
        print(f"   FPS: {dataset.fps}")
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 5. æ£€æŸ¥ features å®šä¹‰
    print_subsection("Features å®šä¹‰ (ä»æ•°æ®é›†å¯¹è±¡)")
    
    print(f"\n   æ•°æ®é›†çš„ features å±æ€§:")
    if hasattr(dataset, 'features'):
        for key, feature in dataset.features.items():
            print(f"\n      ğŸ”‘ {key}:")
            print(f"         ç±»å‹: {type(feature).__name__}")
            print(f"         è¯¦æƒ…: {feature}")
    
    # 6. æ£€æŸ¥ HuggingFace Dataset çš„ features
    print_subsection("HuggingFace Dataset Features")
    
    hf_dataset = dataset.hf_dataset
    print(f"\n   HF Dataset features:")
    for key, feature in hf_dataset.features.items():
        print(f"\n      ğŸ”‘ {key}:")
        print(f"         ç±»å‹: {type(feature).__name__}")
        print(f"         è¯¦æƒ…: {feature}")
    
    # 7. æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®é™…æ•°æ®
    print_subsection("æ ·æœ¬æ•°æ®æ£€æŸ¥ (å‰3ä¸ªæ ·æœ¬)")
    
    for sample_idx in range(min(3, len(dataset))):
        print(f"\n   ğŸ“¦ æ ·æœ¬ #{sample_idx}:")
        
        try:
            sample = dataset[sample_idx]
            
            # æ˜¾ç¤ºæ‰€æœ‰é”®
            print(f"      é”®: {list(sample.keys())}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„è¯¦ç»†ä¿¡æ¯
            for key, value in sample.items():
                print(f"\n      ğŸ”¹ {key}:")
                print(f"         ç±»å‹: {type(value)}")
                
                if isinstance(value, (np.ndarray, np.generic)):
                    print(f"         å½¢çŠ¶: {value.shape}")
                    print(f"         dtype: {value.dtype}")
                    print(f"         å€¼èŒƒå›´: [{np.min(value)}, {np.max(value)}]")
                    if value.size <= 20:
                        print(f"         å€¼: {value}")
                elif hasattr(value, 'shape'):  # torch.Tensor
                    print(f"         å½¢çŠ¶: {value.shape}")
                    print(f"         dtype: {value.dtype}")
                    print(f"         å€¼èŒƒå›´: [{value.min().item()}, {value.max().item()}]")
                    if value.numel() <= 20:
                        print(f"         å€¼: {value}")
                elif isinstance(value, dict):
                    print(f"         å­é”®: {list(value.keys())}")
                    for sub_key, sub_value in value.items():
                        print(f"            â€¢ {sub_key}: {type(sub_value)}", end="")
                        if hasattr(sub_value, 'shape'):
                            print(f" shape={sub_value.shape} dtype={sub_value.dtype}")
                        else:
                            print()
                else:
                    print(f"         å€¼: {value}")
                    
        except Exception as e:
            print(f"      âŒ è¯»å–æ ·æœ¬å¤±è´¥: {e}")
    
    # 8. æ£€æŸ¥ç‰¹å®šç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯
    print_subsection("å›¾åƒç‰¹å¾è¯¦ç»†æ£€æŸ¥")
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒç‰¹å¾
    image_keys = []
    sample = dataset[0]
    
    def find_image_keys(d, prefix=""):
        """é€’å½’æŸ¥æ‰¾å›¾åƒé”®"""
        for key, value in d.items():
            full_key = f"{prefix}.{key}" if prefix else key
            if isinstance(value, dict):
                find_image_keys(value, full_key)
            elif hasattr(value, 'shape') and len(value.shape) == 3:
                # å¯èƒ½æ˜¯å›¾åƒ
                image_keys.append(full_key)
    
    find_image_keys(sample)
    
    if image_keys:
        print(f"\n   æ‰¾åˆ° {len(image_keys)} ä¸ªå¯èƒ½çš„å›¾åƒç‰¹å¾:")
        for img_key in image_keys:
            print(f"\n      ğŸ–¼ï¸  {img_key}:")
            
            # è·å–è¯¥ç‰¹å¾çš„å®é™…æ•°æ®
            keys = img_key.split('.')
            value = sample
            for k in keys:
                value = value[k]
            
            print(f"         å½¢çŠ¶: {value.shape}")
            print(f"         dtype: {value.dtype}")
            print(f"         å€¼èŒƒå›´: [{value.min()}, {value.max()}]")
            
            # åˆ¤æ–­é€šé“é¡ºåº
            shape = value.shape
            if shape[0] == 3 or shape[0] == 1:
                print(f"         é€šé“é¡ºåº: å¯èƒ½æ˜¯ CHW (Channels, Height, Width)")
            elif shape[2] == 3 or shape[2] == 1:
                print(f"         é€šé“é¡ºåº: å¯èƒ½æ˜¯ HWC (Height, Width, Channels)")
            else:
                print(f"         é€šé“é¡ºåº: æœªçŸ¥")
    
    # 9. æ£€æŸ¥ task ç›¸å…³å­—æ®µ
    print_subsection("Task ç›¸å…³å­—æ®µæ£€æŸ¥")
    
    sample = dataset[0]
    
    task_related = ['task', 'task_index', 'language_instruction']
    found_task_fields = []
    
    for field in task_related:
        if field in sample:
            found_task_fields.append(field)
            print(f"\n   âœ… æ‰¾åˆ°å­—æ®µ: {field}")
            print(f"      ç±»å‹: {type(sample[field])}")
            print(f"      å€¼: {sample[field]}")
    
    if not found_task_fields:
        print(f"\n   âš ï¸  æœªæ‰¾åˆ°ä»»ä½• task ç›¸å…³å­—æ®µ")
        print(f"   æ‰€æœ‰å­—æ®µ: {list(sample.keys())}")
    
    # 10. å¯¹æ¯” features å®šä¹‰å’Œå®é™…æ•°æ®
    print_subsection("ç‰¹å¾å®šä¹‰ vs å®é™…æ•°æ®å¯¹æ¯”")
    
    print(f"\n   æ£€æŸ¥ç‰¹å¾ä¸€è‡´æ€§:")
    
    # ä» info.json è·å–æœŸæœ›çš„ features
    expected_features = set()
    if info_path.exists():
        with open(info_path, 'r') as f:
            info = json.load(f)
        if 'features' in info:
            expected_features = set(info['features'].keys())
    
    # ä»å®é™…æ•°æ®è·å– features
    actual_features = set(sample.keys())
    
    print(f"\n   æœŸæœ›çš„ç‰¹å¾ (æ¥è‡ª meta/info.json): {len(expected_features)}")
    for f in sorted(expected_features):
        print(f"      â€¢ {f}")
    
    print(f"\n   å®é™…çš„ç‰¹å¾ (æ¥è‡ªæ•°æ®æ ·æœ¬): {len(actual_features)}")
    for f in sorted(actual_features):
        print(f"      â€¢ {f}")
    
    missing = expected_features - actual_features
    extra = actual_features - expected_features
    
    if missing:
        print(f"\n   âš ï¸  ç¼ºå°‘çš„ç‰¹å¾: {missing}")
    
    if extra:
        print(f"\n   âš ï¸  é¢å¤–çš„ç‰¹å¾: {extra}")
    
    if not missing and not extra:
        print(f"\n   âœ… ç‰¹å¾å®Œå…¨åŒ¹é…")
    
    # 11. æ£€æŸ¥å›¾åƒå½¢çŠ¶ä¸€è‡´æ€§
    print_subsection("å›¾åƒå½¢çŠ¶ä¸€è‡´æ€§æ£€æŸ¥")
    
    for img_key in image_keys[:3]:  # åªæ£€æŸ¥å‰3ä¸ªå›¾åƒç‰¹å¾
        print(f"\n   æ£€æŸ¥ {img_key}:")
        
        # ä» features å®šä¹‰è·å–æœŸæœ›å½¢çŠ¶
        if hasattr(dataset, 'features'):
            keys = img_key.split('.')
            feature_def = dataset.features
            try:
                for k in keys:
                    if hasattr(feature_def, k):
                        feature_def = getattr(feature_def, k)
                    elif isinstance(feature_def, dict):
                        feature_def = feature_def[k]
                
                if hasattr(feature_def, 'shape'):
                    expected_shape = feature_def.shape
                    print(f"      æœŸæœ›å½¢çŠ¶ (ä» features): {expected_shape}")
                else:
                    print(f"      æœŸæœ›å½¢çŠ¶: æœªå®šä¹‰")
            except:
                print(f"      æœŸæœ›å½¢çŠ¶: æ— æ³•è·å–")
        
        # è·å–å®é™…å½¢çŠ¶
        keys = img_key.split('.')
        value = sample
        for k in keys:
            value = value[k]
        
        actual_shape = value.shape
        print(f"      å®é™…å½¢çŠ¶: {actual_shape}")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬æ¢
        if len(actual_shape) == 3:
            if actual_shape[0] in [1, 3]:
                print(f"      ğŸ’¡ å»ºè®®: å¯èƒ½éœ€è¦ä» CHW è½¬æ¢ä¸º HWC")
                print(f"         è½¬æ¢åå½¢çŠ¶: {(actual_shape[1], actual_shape[2], actual_shape[0])}")
            elif actual_shape[2] in [1, 3]:
                print(f"      âœ… å·²ç»æ˜¯ HWC æ ¼å¼")
    
    print_section("æ£€æŸ¥å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(
        description="æ£€æŸ¥ LeRobot æ•°æ®é›†çš„è¯¦ç»†ç»“æ„"
    )
    
    parser.add_argument(
        "--repo_id",
        type=str,
        default="Coil1987121/calvin_lerobot_task_ABCD_D_training",
        help="æ•°æ®é›†çš„ repo_id"
    )
    
    args = parser.parse_args()
    
    inspect_dataset_structure(args.repo_id)


if __name__ == "__main__":
    main()