#!/usr/bin/env python3
"""
éªŒè¯åˆ›å»ºçš„å­æ•°æ®é›†

Usage:
    python verify_subset.py Coil1987121/calvin_5percent_seed42_training
"""

import sys
import json
from pathlib import Path
from collections import Counter


def verify_subset(repo_id: str):
    """éªŒè¯å­æ•°æ®é›†çš„å®Œæ•´æ€§å’Œç»Ÿè®¡ä¿¡æ¯"""
    
    print("=" * 70)
    print(f"ğŸ” éªŒè¯æ•°æ®é›†: {repo_id}")
    print("=" * 70)
    
    try:
        from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME
        cache_dir = HF_LEROBOT_HOME
    except ImportError:
        from huggingface_hub import constants
        cache_dir = Path(constants.HF_HOME) / "lerobot"
    
    dataset_path = cache_dir / repo_id
    
    # 1. æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    print(f"\nğŸ“ æ£€æŸ¥è·¯å¾„...")
    if not dataset_path.exists():
        print(f"   âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        print(f"\nğŸ’¡ æç¤º:")
        print(f"   1. è¿è¡Œ 'python create_subset_hf_cache.py --list' æŸ¥çœ‹æ‰€æœ‰æ•°æ®é›†")
        print(f"   2. ç¡®ä¿å·²åˆ›å»ºè¯¥å­æ•°æ®é›†")
        return False
    print(f"   âœ… è·¯å¾„å­˜åœ¨: {dataset_path}")
    
    # 2. æ£€æŸ¥åŸºæœ¬ç»“æ„
    print(f"\nğŸ“‚ æ£€æŸ¥ç›®å½•ç»“æ„...")
    required_dirs = ["data", "meta"]
    for dir_name in required_dirs:
        dir_path = dataset_path / dir_name
        if dir_path.exists():
            print(f"   âœ… {dir_name}/ å­˜åœ¨")
        else:
            print(f"   âŒ {dir_name}/ ç¼ºå¤±")
    
    # 3. è¯»å–å…ƒæ•°æ®
    print(f"\nğŸ“Š è¯»å–å…ƒæ•°æ®...")
    
    # å­é›†å…ƒæ•°æ®
    subset_meta_path = dataset_path / "subset_metadata.json"
    if subset_meta_path.exists():
        with open(subset_meta_path, 'r') as f:
            subset_meta = json.load(f)
        
        print(f"\n   ğŸ¯ é‡‡æ ·ä¿¡æ¯:")
        print(f"   æºæ•°æ®é›†: {subset_meta.get('source_repo_id', 'N/A')}")
        print(f"   é‡‡æ ·æ¯”ä¾‹: {subset_meta.get('fraction', 0)*100:.1f}%")
        print(f"   éšæœºç§å­: {subset_meta.get('seed', 'N/A')}")
        print(f"\n   ğŸ“ˆ ç»Ÿè®¡:")
        print(f"   Episodes: {subset_meta.get('total_episodes', '?')} / {subset_meta.get('original_episodes', '?')}")
        print(f"   Frames: {subset_meta.get('total_frames', '?')} / {subset_meta.get('original_frames', '?')}")
        print(f"   å®é™…æ¯”ä¾‹: {subset_meta.get('actual_fraction', 0)*100:.2f}%")
        
        sampled_episodes = subset_meta.get('sampled_episode_ids', [])
        if sampled_episodes:
            print(f"\n   ğŸ² é‡‡æ ·çš„ episodes (å‰20ä¸ª):")
            print(f"   {sampled_episodes[:20]}")
    else:
        print(f"   âš ï¸  æœªæ‰¾åˆ° subset_metadata.json")
    
    # æ•°æ®é›†ä¿¡æ¯
    info_path = dataset_path / "meta" / "info.json"
    if info_path.exists():
        with open(info_path, 'r') as f:
            info = json.load(f)
        
        print(f"\n   â„¹ï¸  æ•°æ®é›†ä¿¡æ¯:")
        print(f"   Episodes: {info.get('total_episodes', '?')}")
        print(f"   Frames: {info.get('total_frames', '?')}")
        print(f"   FPS: {info.get('fps', '?')}")
        print(f"   Robot: {info.get('robot_type', '?')}")
    
    # 4. åŠ è½½å¹¶éªŒè¯æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½æ•°æ®é›†...")
    try:
        from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset
        import os
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è®¿é—® HuggingFace Hub
        old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        try:
            # ä½¿ç”¨ LeRobotDataset åŠ è½½
            print(f"   â³ æ­£åœ¨åŠ è½½...", end=" ", flush=True)
            dataset = LeRobotDataset(repo_id, root=HF_LEROBOT_HOME, local_files_only=True)
            print(f"âœ…")
        finally:
            # æ¢å¤ç¯å¢ƒå˜é‡
            if old_hf_hub_offline is None:
                os.environ.pop('HF_HUB_OFFLINE', None)
            else:
                os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
        
        print(f"\n   ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ€»å¸§æ•°: {len(dataset)}")
        print(f"   æ€» episodes: {dataset.num_episodes}")
        print(f"   FPS: {dataset.fps}")
        print(f"   Robot: {dataset.robot_type}")
        
        # è·å–åº•å±‚çš„ HuggingFace Dataset
        hf_dataset = dataset.hf_dataset
        print(f"\n   ğŸ“‹ æ•°æ®å­—æ®µ: {hf_dataset.column_names}")
        
        # ç»Ÿè®¡ episodes
        episodes = sorted(set(hf_dataset['episode_index']))
        print(f"\n   ğŸ“ˆ Episode ä¿¡æ¯:")
        print(f"   å”¯ä¸€ episodes: {len(episodes)}")
        print(f"   Episode ID èŒƒå›´: {min(episodes)} - {max(episodes)}")
        
        # æ¯ä¸ª episode çš„å¸§æ•°
        episode_counts = Counter(hf_dataset['episode_index'])
        frames_per_ep = list(episode_counts.values())
        print(f"   æ¯ä¸ª episode å¹³å‡å¸§æ•°: {sum(frames_per_ep)/len(frames_per_ep):.1f}")
        print(f"   å¸§æ•°èŒƒå›´: {min(frames_per_ep)} - {max(frames_per_ep)}")
        
        # ä»»åŠ¡åˆ†å¸ƒ
        if 'task' in hf_dataset.column_names:
            tasks = [sample['task'] for sample in hf_dataset.select(range(min(1000, len(hf_dataset))))]
            task_counts = Counter(tasks)
            print(f"\n   ğŸ“ ä»»åŠ¡åˆ†å¸ƒ (å‰ 10 ä¸ªä»»åŠ¡ï¼ŒåŸºäºå‰ 1000 æ ·æœ¬):")
            for task, count in task_counts.most_common(10):
                task_str = task[:60] + "..." if len(task) > 60 else task
                print(f"      {count:5d} x {task_str}")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
        print(f"\n   ğŸ” ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å­—æ®µ:")
        first_sample = hf_dataset[0]
        for key, value in sorted(first_sample.items()):
            if hasattr(value, 'shape'):
                print(f"      {key}: shape={value.shape}, dtype={value.dtype}")
            elif hasattr(value, '__len__') and not isinstance(value, str):
                print(f"      {key}: length={len(value)}, type={type(value).__name__}")
            else:
                value_str = str(value)[:60]
                if len(str(value)) > 60:
                    value_str += "..."
                print(f"      {key}: {value_str}")
        
        # Features ä¿¡æ¯
        print(f"\n   ğŸ”§ æ•°æ®é›† Features:")
        for key, feature in dataset.features.items():
            print(f"      {key}:")
            print(f"         dtype: {feature.get('dtype', 'N/A')}")
            print(f"         shape: {feature.get('shape', 'N/A')}")
        
    except ImportError:
        print(f"\n   âŒ åŠ è½½å¤±è´¥: lerobot åº“æœªå®‰è£…")
        print(f"   ğŸ’¡ è¯·è¿è¡Œ: pip install lerobot")
        return False
    except Exception as e:
        print(f"\n   âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nğŸ’¡ æ•…éšœæ’æŸ¥:")
        print(f"   1. æ£€æŸ¥ meta/info.json æ˜¯å¦å­˜åœ¨: {dataset_path}/meta/info.json")
        print(f"   2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰")
        print(f"   3. ç¡®ä¿æ•°æ®é›†ç»“æ„å®Œæ•´")
        return False
    
    # 5. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… éªŒè¯å®Œæˆï¼")
    print("=" * 70)
    
    print(f"\nğŸ’¡ åœ¨è®­ç»ƒé…ç½®ä¸­ä½¿ç”¨:")
    print(f"```python")
    print(f"repo_id=\"{repo_id}\"")
    print(f"```")
    
    return True


def compare_subsets(repo_ids: list):
    """æ¯”è¾ƒå¤šä¸ªå­æ•°æ®é›†"""
    
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”å­æ•°æ®é›†")
    print("=" * 70)
    
    try:
        from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset
        import os
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… lerobot åº“")
        return
    
    datasets_info = []
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    old_hf_hub_offline = os.environ.get('HF_HUB_OFFLINE')
    os.environ['HF_HUB_OFFLINE'] = '1'
    
    try:
        for repo_id in repo_ids:
            dataset_path = HF_LEROBOT_HOME / repo_id
            if not dataset_path.exists():
                print(f"\nâš ï¸  è·³è¿‡ {repo_id} (ä¸å­˜åœ¨)")
                continue
            
            try:
                print(f"\nâ³ åŠ è½½ {repo_id}...", end=" ", flush=True)
                dataset = LeRobotDataset(repo_id, root=HF_LEROBOT_HOME, local_files_only=True)
                print("âœ…")
                
                # è¯»å–å…ƒæ•°æ®
                subset_meta_path = dataset_path / "subset_metadata.json"
                if subset_meta_path.exists():
                    with open(subset_meta_path, 'r') as f:
                        meta = json.load(f)
                else:
                    meta = {}
                
                episodes = set(dataset.hf_dataset['episode_index'])
                
                info = {
                    'repo_id': repo_id,
                    'episodes': len(episodes),
                    'frames': len(dataset),
                    'fraction': meta.get('fraction', '?'),
                    'seed': meta.get('seed', '?'),
                    'episode_ids': episodes
                }
                datasets_info.append(info)
                
            except Exception as e:
                print(f"\nâš ï¸  åŠ è½½ {repo_id} å¤±è´¥: {e}")
    finally:
        # æ¢å¤ç¯å¢ƒå˜é‡
        if old_hf_hub_offline is None:
            os.environ.pop('HF_HUB_OFFLINE', None)
        else:
            os.environ['HF_HUB_OFFLINE'] = old_hf_hub_offline
    
    if not datasets_info:
        print("\næ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†")
        return
    
    # æ‰“å°å¯¹æ¯”è¡¨
    print(f"\n{'Repo ID':<50} {'Episodes':>10} {'Frames':>10} {'Fraction':>10} {'Seed':>8}")
    print("-" * 90)
    for info in datasets_info:
        fraction_str = f"{info['fraction']*100:.1f}%" if isinstance(info['fraction'], float) else str(info['fraction'])
        print(f"{info['repo_id']:<50} {info['episodes']:>10} {info['frames']:>10} {fraction_str:>10} {info['seed']:>8}")
    
    # æ£€æŸ¥é‡å 
    if len(datasets_info) >= 2:
        print(f"\nğŸ”„ Episode é‡å åˆ†æ:")
        for i in range(len(datasets_info)):
            for j in range(i+1, len(datasets_info)):
                name1 = datasets_info[i]['repo_id'].split('/')[-1]
                name2 = datasets_info[j]['repo_id'].split('/')[-1]
                
                eps1 = datasets_info[i]['episode_ids']
                eps2 = datasets_info[j]['episode_ids']
                
                overlap = eps1 & eps2
                overlap_pct = len(overlap) / min(len(eps1), len(eps2)) * 100 if min(len(eps1), len(eps2)) > 0 else 0
                
                print(f"\n   {name1} vs {name2}:")
                print(f"   é‡å  episodes: {len(overlap)} ({overlap_pct:.1f}%)")
                if len(overlap) > 0 and len(overlap) <= 20:
                    print(f"   é‡å çš„ episode IDs: {sorted(list(overlap))}")


def main():
    if len(sys.argv) < 2:
        print("Usage: python verify_subset.py <repo_id> [repo_id2] [repo_id3] ...")
        print("\nExamples:")
        print("  # éªŒè¯å•ä¸ªæ•°æ®é›†")
        print("  python verify_subset.py Coil1987121/calvin_5percent_seed42_training")
        print()
        print("  # å¯¹æ¯”å¤šä¸ªæ•°æ®é›†")
        print("  python verify_subset.py \\")
        print("      Coil1987121/calvin_5percent_seed42_training \\")
        print("      Coil1987121/calvin_10percent_seed42_training \\")
        print("      Coil1987121/calvin_5percent_seed123_training")
        sys.exit(1)
    
    repo_ids = sys.argv[1:]
    
    if len(repo_ids) == 1:
        # å•ä¸ªæ•°æ®é›†éªŒè¯
        verify_subset(repo_ids[0])
    else:
        # å¤šä¸ªæ•°æ®é›†å¯¹æ¯”
        for repo_id in repo_ids:
            print()
            verify_subset(repo_id)
        
        # å¯¹æ¯”åˆ†æ
        compare_subsets(repo_ids)


if __name__ == "__main__":
    main()