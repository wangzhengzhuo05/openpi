#!/usr/bin/env python3
"""
Calvinæ•°æ®é›†å¤„ç†å™¨ - ä¿®æ­£ç‰ˆï¼ˆæ­£ç¡®çš„è§¦è§‰é”®åï¼‰
æ”¯æŒå®Œæ•´çš„ä¼ æ„Ÿå™¨å¥—ä»¶ï¼ŒåŒ…æ‹¬è§¦è§‰RGBå’Œè§¦è§‰æ·±åº¦

ä½¿ç”¨æ–¹æ³•:
    python process_calvin_fixed.py --dataset_path <è·¯å¾„> --include_tactile
"""

import numpy as np
import json
from pathlib import Path
from PIL import Image
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import argparse
import cv2


class CalvinDatasetProcessor:
    """Calvinæ•°æ®é›†å¤„ç†å™¨ - æ”¯æŒå®Œæ•´ä¼ æ„Ÿå™¨å¥—ä»¶ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    # Calvinä¼ æ„Ÿå™¨è§„æ ¼ï¼ˆåŸºäºå®é™…æ•°æ®ï¼‰
    SENSOR_SPECS = {
        'rgb_static': {'shape': (200, 200, 3), 'type': 'image'},
        'depth_static': {'shape': (200, 200), 'type': 'depth'},
        'rgb_gripper': {'shape': (84, 84, 3), 'type': 'image'},
        'depth_gripper': {'shape': (84, 84), 'type': 'depth'},
        'rgb_tactile': {'shape': (160, 120, 6), 'type': 'tactile'},      # â­ ä¿®æ­£é”®å
        'depth_tactile': {'shape': (160, 120, 2), 'type': 'tactile'},    # â­ æ–°å¢
        'robot_obs': {'shape': (15,), 'type': 'state'},
        'scene_obs': {'shape': (24,), 'type': 'state'},
        'actions': {'shape': (7,), 'type': 'action'},
        'rel_actions': {'shape': (7,), 'type': 'action'},
    }
    
    def __init__(self, dataset_path: str, output_path: str, 
                 include_tactile: bool = False,
                 include_gripper_cam: bool = True):
        self.dataset_path = Path(dataset_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        self.include_tactile = include_tactile
        self.include_gripper_cam = include_gripper_cam
        
        print(f"ğŸ“Š ä¼ æ„Ÿå™¨é…ç½®:")
        print(f"   â€¢ Static Camera RGB: âœ…")
        print(f"   â€¢ Static Camera Depth: âœ…")
        print(f"   â€¢ Gripper Camera RGB: {'âœ…' if include_gripper_cam else 'âŒ'}")
        print(f"   â€¢ Gripper Camera Depth: {'âœ…' if include_gripper_cam else 'âŒ'}")
        print(f"   â€¢ Tactile RGB (160x120x6): {'âœ…' if include_tactile else 'âŒ'}")
        print(f"   â€¢ Tactile Depth (160x120x2): {'âœ…' if include_tactile else 'âŒ'}")
        
    def load_language_annotations(self) -> Dict:
        """åŠ è½½è¯­è¨€æ ‡æ³¨ - æ”¯æŒæ‰å¹³åŒ–æ ¼å¼"""
        lang_folder = self.dataset_path / 'lang_annotations'
        
        if not lang_folder.exists():
            print(f"âš ï¸  æœªæ‰¾åˆ° lang_annotations æ–‡ä»¶å¤¹")
            return {}
        
        auto_lang_file = lang_folder / 'auto_lang_ann.npy'
        if not auto_lang_file.exists():
            print(f"âš ï¸  æœªæ‰¾åˆ° auto_lang_ann.npy")
            return {}
        
        print(f"âœ… åŠ è½½è¯­è¨€æ ‡æ³¨: auto_lang_ann.npy")
        data = np.load(auto_lang_file, allow_pickle=True).item()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰å¹³åŒ–æ ¼å¼
        if 'language' in data and 'info' in data:
            return self._parse_flat_annotations(data)
        else:
            return data
    
    def _parse_flat_annotations(self, data: Dict) -> Dict:
        """è§£ææ‰å¹³åŒ–çš„æ ‡æ³¨æ ¼å¼"""
        annotations = {}
        
        language = data.get('language', {})
        info = data.get('info', {})
        
        anns = language.get('ann', [])
        tasks = language.get('task', [])
        indxs = info.get('indx', [])
        
        print(f"ğŸ“Š è§£æåˆ° {len(anns)} ä¸ªè¯­è¨€æ ‡æ³¨episodes")
        
        for i, (ann, task, (start, end)) in enumerate(zip(anns, tasks, indxs)):
            episode_id = f"lang_ann_{i:04d}"
            annotations[episode_id] = {
                'start_idx': int(start),
                'end_idx': int(end),
                'language': ann,
                'task': task
            }
            
        return annotations
    
    def find_episode_sequences(self) -> Dict[str, Dict]:
        """æŸ¥æ‰¾å¹¶ç»„ç»‡episodeåºåˆ— - åªä½¿ç”¨è¯­è¨€æ ‡æ³¨"""
        npz_files = sorted(self.dataset_path.glob('episode_*.npz'))
        
        if not npz_files:
            print(f"âš ï¸  åœ¨ {self.dataset_path} ä¸­æœªæ‰¾åˆ°episodeæ–‡ä»¶")
            return {}
        
        print(f"âœ… æ‰¾åˆ° {len(npz_files)} ä¸ªepisodeå¸§æ–‡ä»¶")
        
        lang_annotations = self.load_language_annotations()
        
        if not lang_annotations:
            print("âŒ æœªæ‰¾åˆ°è¯­è¨€æ ‡æ³¨")
            return {}
        
        episodes = {}
        for episode_id, ann_data in lang_annotations.items():
            episodes[episode_id] = {
                'start': ann_data['start_idx'],
                'end': ann_data['end_idx'],
                'language_instruction': ann_data['language'],
                'task': ann_data.get('task', '')
            }
        
        print(f"âœ… å°†å¤„ç† {len(episodes)} ä¸ªepisodesï¼ˆä»…åŒ…å«è¯­è¨€æ ‡æ³¨çš„episodesï¼‰")
        return episodes
    
    def load_episode_data(self, start_idx: int, end_idx: int) -> Dict:
        """åŠ è½½ä¸€ä¸ªepisodeçš„æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬è§¦è§‰RGBå’Œè§¦è§‰æ·±åº¦ï¼‰"""
        robot_observations = []
        actions = []
        relative_actions = []
        scene_observations = []
        tactile_rgb_observations = []      # â­ RGBè§¦è§‰
        tactile_depth_observations = []    # â­ æ·±åº¦è§¦è§‰
        missing_frames = []
        
        # ä¼ æ„Ÿå™¨å¯ç”¨æ€§ç»Ÿè®¡
        sensor_availability = {
            'rgb_static': 0,
            'depth_static': 0,
            'rgb_gripper': 0,
            'depth_gripper': 0,
            'rgb_tactile': 0,      # â­ ä¿®æ­£é”®å
            'depth_tactile': 0,    # â­ æ–°å¢
            'robot_obs': 0,
            'actions': 0,
            'scene_obs': 0
        }
        
        for frame_idx in range(start_idx, end_idx + 1):
            npz_file = self.dataset_path / f'episode_{frame_idx:07d}.npz'
            
            if not npz_file.exists():
                missing_frames.append(frame_idx)
                continue
            
            try:
                data = np.load(npz_file, allow_pickle=True)
                
                # æœºå™¨äººçŠ¶æ€
                if 'robot_obs' in data:
                    robot_observations.append(data['robot_obs'].tolist())
                    sensor_availability['robot_obs'] += 1
                
                # åŠ¨ä½œ
                if 'actions' in data:
                    actions.append(data['actions'].tolist())
                    sensor_availability['actions'] += 1
                
                if 'rel_actions' in data:
                    relative_actions.append(data['rel_actions'].tolist())
                
                # åœºæ™¯è§‚æµ‹
                if 'scene_obs' in data:
                    scene_observations.append(data['scene_obs'].tolist())
                    sensor_availability['scene_obs'] += 1
                
                # â­ RGBè§¦è§‰æ•°æ®ï¼ˆä¿®æ­£é”®åï¼‰
                if self.include_tactile and 'rgb_tactile' in data:
                    tactile_rgb_observations.append(data['rgb_tactile'])
                    sensor_availability['rgb_tactile'] += 1
                
                # â­ æ·±åº¦è§¦è§‰æ•°æ®ï¼ˆæ–°å¢ï¼‰
                if self.include_tactile and 'depth_tactile' in data:
                    tactile_depth_observations.append(data['depth_tactile'])
                    sensor_availability['depth_tactile'] += 1
                
                # æ£€æŸ¥å…¶ä»–ä¼ æ„Ÿå™¨å¯ç”¨æ€§
                for sensor in ['rgb_static', 'depth_static', 'rgb_gripper', 'depth_gripper']:
                    if sensor in data:
                        sensor_availability[sensor] += 1
                    
            except Exception as e:
                print(f"âš ï¸  è¯»å– {npz_file} å¤±è´¥: {e}")
                missing_frames.append(frame_idx)
        
        return {
            'robot_observations': robot_observations,
            'actions': actions,
            'relative_actions': relative_actions,
            'scene_observations': scene_observations,
            'tactile_rgb_observations': tactile_rgb_observations,      # â­ RGBè§¦è§‰
            'tactile_depth_observations': tactile_depth_observations,  # â­ æ·±åº¦è§¦è§‰
            'expected_count': end_idx - start_idx + 1,
            'actual_count': len(robot_observations),
            'missing_count': len(missing_frames),
            'missing_frames': missing_frames,
            'sensor_availability': sensor_availability
        }
    
    def _normalize_depth(self, depth_array: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–æ·±åº¦å›¾ - ä¿è¯è¿”å›uint8"""
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        if depth_array.size == 0:
            return np.zeros_like(depth_array, dtype=np.uint8)
        
        # åªè€ƒè™‘æœ‰æ•ˆæ·±åº¦å€¼
        valid_depth = depth_array[depth_array > 0]
        
        if len(valid_depth) == 0:
            # å…¨æ˜¯æ— æ•ˆå€¼ï¼Œè¿”å›å…¨é›¶
            return np.zeros_like(depth_array, dtype=np.uint8)
        
        # ä½¿ç”¨ç™¾åˆ†ä½æ•°é¿å…å¼‚å¸¸å€¼
        min_depth = np.percentile(valid_depth, 1)
        max_depth = np.percentile(valid_depth, 99)
        
        # å½’ä¸€åŒ–åˆ°0-1
        if max_depth > min_depth:
            normalized = (depth_array - min_depth) / (max_depth - min_depth + 1e-8)
            normalized = np.clip(normalized, 0, 1)
        else:
            normalized = np.zeros_like(depth_array, dtype=np.float32)
        
        # è½¬æ¢åˆ°0-255çš„uint8
        result = (normalized * 255).astype(np.uint8)
        
        return result

    def _process_tactile_depth(self, tactile_depth: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """å¤„ç†è§¦è§‰æ·±åº¦æ•°æ® - ç¡®ä¿è¿”å›uint8"""
        ch0 = tactile_depth[:, :, 0]
        ch1 = tactile_depth[:, :, 1]
        
        # å½’ä¸€åŒ–æ¯ä¸ªé€šé“
        ch0_viz = self._normalize_depth(ch0)
        ch1_viz = self._normalize_depth(ch1)
        
        # åŒé‡ä¿é™©ï¼šç¡®ä¿æ˜¯uint8
        assert ch0_viz.dtype == np.uint8, f"ch0ç±»å‹é”™è¯¯: {ch0_viz.dtype}"
        assert ch1_viz.dtype == np.uint8, f"ch1ç±»å‹é”™è¯¯: {ch1_viz.dtype}"
        
        return ch0_viz, ch1_viz
        
    def _process_tactile_rgb_for_visualization(self, tactile_array: np.ndarray) -> np.ndarray:
        """å¤„ç†è§¦è§‰RGBå›¾åƒç”¨äºå¯è§†åŒ–
        
        è§¦è§‰RGBæ˜¯160x120x6ï¼ŒåŒ…å«6ä¸ªé€šé“
        å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†RGBå›¾åƒ
        
        Args:
            tactile_array: (160, 120, 6) è§¦è§‰RGBæ•°æ®
        
        Returns:
            rgb_image: (160, 120, 3) RGBå›¾åƒ
        """
        # æ–¹æ³•: ä½¿ç”¨å‰3ä¸ªé€šé“
        if tactile_array.shape[2] >= 3:
            rgb = tactile_array[:, :, :3]
        else:
            rgb = np.stack([tactile_array[:, :, 0]] * 3, axis=-1)
        
        # ç¡®ä¿æ˜¯uint8
        if rgb.dtype != np.uint8:
            rgb = rgb.astype(np.uint8)
        
        return rgb
    
    def _create_tactile_heatmap(self, tactile_array: np.ndarray) -> np.ndarray:
        """åˆ›å»ºè§¦è§‰çƒ­å›¾ï¼ˆæ‰€æœ‰6ä¸ªé€šé“çš„å¹³å‡ï¼‰
        
        Args:
            tactile_array: (160, 120, 6)
        
        Returns:
            heatmap: (160, 120, 3) RGBçƒ­å›¾
        """
        # è®¡ç®—æ‰€æœ‰é€šé“çš„å¹³å‡
        tactile_mean = tactile_array.mean(axis=2)
        
        # å½’ä¸€åŒ–
        tactile_norm = (tactile_mean - tactile_mean.min()) / (tactile_mean.max() - tactile_mean.min() + 1e-8)
        tactile_norm = (tactile_norm * 255).astype(np.uint8)
        
        # åº”ç”¨é¢œè‰²æ˜ å°„
        heatmap = cv2.applyColorMap(tactile_norm, cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        return heatmap
    
    def save_episode_images(self, episode_id: str, start_idx: int, end_idx: int) -> Dict:
        """ä¿å­˜episodeçš„å›¾åƒã€æ·±åº¦å›¾å’Œè§¦è§‰æ•°æ®"""
        image_folder = self.output_path / 'images' / episode_id
        image_folder.mkdir(parents=True, exist_ok=True)
        
        folders = {
            'rgb_static': image_folder / 'rgb_static',
            'depth_static': image_folder / 'depth_static',
        }
        
        if self.include_gripper_cam:
            folders['rgb_gripper'] = image_folder / 'rgb_gripper'
            folders['depth_gripper'] = image_folder / 'depth_gripper'
        
        if self.include_tactile:
            # RGBè§¦è§‰
            folders['tactile_rgb'] = image_folder / 'tactile_rgb'              # åŸå§‹6é€šé“
            folders['tactile_rgb_viz'] = image_folder / 'tactile_rgb_viz'      # RGBå¯è§†åŒ–
            folders['tactile_rgb_heatmap'] = image_folder / 'tactile_rgb_heatmap'  # çƒ­å›¾
            
            # æ·±åº¦è§¦è§‰
            folders['tactile_depth'] = image_folder / 'tactile_depth'          # åŸå§‹2é€šé“
            folders['tactile_depth_ch0'] = image_folder / 'tactile_depth_ch0'  # é€šé“0
            folders['tactile_depth_ch1'] = image_folder / 'tactile_depth_ch1'  # é€šé“1
        
        for folder in folders.values():
            folder.mkdir(exist_ok=True)
        
        counts = {k: 0 for k in folders.keys()}
        
        for frame_idx in range(start_idx, end_idx + 1):
            npz_file = self.dataset_path / f'episode_{frame_idx:07d}.npz'
            if not npz_file.exists():
                continue
            
            try:
                data = np.load(npz_file, allow_pickle=True)
                
                # RGB Static Camera
                if 'rgb_static' in data:
                    Image.fromarray(data['rgb_static']).save(
                        folders['rgb_static'] / f'frame_{frame_idx:07d}.png')
                    counts['rgb_static'] += 1
                
                # Depth Static Camera
                if 'depth_static' in data:
                    depth = data['depth_static']
                    Image.fromarray(self._normalize_depth(depth)).save(
                        folders['depth_static'] / f'frame_{frame_idx:07d}.png')
                    np.save(folders['depth_static'] / f'frame_{frame_idx:07d}.npy', depth)
                    counts['depth_static'] += 1
                
                # RGB Gripper Camera
                if self.include_gripper_cam and 'rgb_gripper' in data:
                    Image.fromarray(data['rgb_gripper']).save(
                        folders['rgb_gripper'] / f'frame_{frame_idx:07d}.png')
                    counts['rgb_gripper'] += 1
                
                # Depth Gripper Camera
                if self.include_gripper_cam and 'depth_gripper' in data:
                    depth = data['depth_gripper']
                    Image.fromarray(self._normalize_depth(depth)).save(
                        folders['depth_gripper'] / f'frame_{frame_idx:07d}.png')
                    np.save(folders['depth_gripper'] / f'frame_{frame_idx:07d}.npy', depth)
                    counts['depth_gripper'] += 1
                
                # â­ RGBè§¦è§‰æ•°æ®ï¼ˆä¿®æ­£é”®åï¼‰
                if self.include_tactile and 'rgb_tactile' in data:
                    tactile_rgb = data['rgb_tactile']
                    
                    # éªŒè¯å½¢çŠ¶
                    expected_shape = self.SENSOR_SPECS['rgb_tactile']['shape']
                    if tactile_rgb.shape != expected_shape:
                        print(f"  âš ï¸  RGBè§¦è§‰å½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {tactile_rgb.shape}")
                    
                    # 1. ä¿å­˜åŸå§‹6é€šé“æ•°æ®ï¼ˆnumpyæ ¼å¼ï¼‰
                    np.save(folders['tactile_rgb'] / f'frame_{frame_idx:07d}.npy', tactile_rgb)
                    
                    # 2. ä¿å­˜RGBå¯è§†åŒ–ï¼ˆå‰3ä¸ªé€šé“ï¼‰
                    tactile_rgb_viz = self._process_tactile_rgb_for_visualization(tactile_rgb)
                    Image.fromarray(tactile_rgb_viz).save(
                        folders['tactile_rgb_viz'] / f'frame_{frame_idx:07d}.png')
                    
                    # 3. ä¿å­˜çƒ­å›¾ï¼ˆæ‰€æœ‰é€šé“çš„å¹³å‡ï¼‰
                    tactile_rgb_heatmap = self._create_tactile_heatmap(tactile_rgb)
                    Image.fromarray(tactile_rgb_heatmap).save(
                        folders['tactile_rgb_heatmap'] / f'frame_{frame_idx:07d}.png')
                    
                    counts['tactile_rgb'] += 1
                    counts['tactile_rgb_viz'] += 1
                    counts['tactile_rgb_heatmap'] += 1
                
                # â­ æ·±åº¦è§¦è§‰æ•°æ®ï¼ˆæ–°å¢ï¼‰
                if self.include_tactile and 'depth_tactile' in data:
                    tactile_depth = data['depth_tactile']
                    
                    # éªŒè¯å½¢çŠ¶
                    expected_shape = self.SENSOR_SPECS['depth_tactile']['shape']
                    if tactile_depth.shape != expected_shape:
                        print(f"  âš ï¸  æ·±åº¦è§¦è§‰å½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {tactile_depth.shape}")
                    
                    # 1. ä¿å­˜åŸå§‹2é€šé“æ•°æ®
                    np.save(folders['tactile_depth'] / f'frame_{frame_idx:07d}.npy', tactile_depth)
                    
                    # 2. åˆ†åˆ«ä¿å­˜ä¸¤ä¸ªé€šé“çš„å¯è§†åŒ–
                    ch0_viz, ch1_viz = self._process_tactile_depth(tactile_depth)
                    
                    Image.fromarray(ch0_viz).save(
                        folders['tactile_depth_ch0'] / f'frame_{frame_idx:07d}.png')
                    Image.fromarray(ch1_viz).save(
                        folders['tactile_depth_ch1'] / f'frame_{frame_idx:07d}.png')
                    
                    counts['tactile_depth'] += 1
                    counts['tactile_depth_ch0'] += 1
                    counts['tactile_depth_ch1'] += 1
                    
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜å›¾åƒå¤±è´¥ {npz_file}: {e}")
                import traceback
                traceback.print_exc()
        
        return {'image_folder': f'images/{episode_id}', **counts}
    
    def process_episode(self, episode_id: str, episode_info: Dict, 
                       save_images: bool = True) -> Dict:
        """å¤„ç†å•ä¸ªepisode"""
        start_idx = episode_info['start']
        end_idx = episode_info['end']
        
        trajectory_data = self.load_episode_data(start_idx, end_idx)
        visual_info = {}
        if save_images:
            visual_info = self.save_episode_images(episode_id, start_idx, end_idx)
        
        # æ•°æ®ç»´åº¦ç»Ÿè®¡
        dims = {'num_frames': trajectory_data['actual_count']}
        
        if trajectory_data['robot_observations']:
            dims['robot_obs_dim'] = len(trajectory_data['robot_observations'][0])
        
        if trajectory_data['actions']:
            dims['actions_dim'] = len(trajectory_data['actions'][0])
        
        if trajectory_data['relative_actions']:
            dims['rel_actions_dim'] = len(trajectory_data['relative_actions'][0])
        
        if trajectory_data['scene_observations']:
            dims['scene_obs_dim'] = len(trajectory_data['scene_observations'][0])
        
        # â­ RGBè§¦è§‰
        if trajectory_data['tactile_rgb_observations']:
            tactile_shape = trajectory_data['tactile_rgb_observations'][0].shape
            dims['tactile_rgb_shape'] = list(tactile_shape)
            dims['has_tactile_rgb'] = True
        else:
            dims['has_tactile_rgb'] = False
        
        # â­ æ·±åº¦è§¦è§‰
        if trajectory_data['tactile_depth_observations']:
            tactile_depth_shape = trajectory_data['tactile_depth_observations'][0].shape
            dims['tactile_depth_shape'] = list(tactile_depth_shape)
            dims['has_tactile_depth'] = True
        else:
            dims['has_tactile_depth'] = False
        
        # ä¼ æ„Ÿå™¨å¯ç”¨æ€§
        sensor_stats = trajectory_data['sensor_availability']
        total_frames = trajectory_data['actual_count']
        
        episode_json = {
            'episode_id': episode_id,
            'language_instruction': episode_info['language_instruction'],
            'task': episode_info.get('task', ''),
            'source_frames': {
                'start': start_idx,
                'end': end_idx,
                'expected_count': trajectory_data['expected_count'],
                'actual_count': trajectory_data['actual_count'],
                'missing_count': trajectory_data['missing_count']
            },
            'trajectory': {
                'robot_observations': trajectory_data['robot_observations'],
                'actions': trajectory_data['actions'],
                'relative_actions': trajectory_data['relative_actions'],
                'scene_observations': trajectory_data['scene_observations']
            },
            'data_statistics': dims,
            'sensor_coverage': {
                'rgb_static': f"{sensor_stats['rgb_static']}/{total_frames}",
                'depth_static': f"{sensor_stats['depth_static']}/{total_frames}",
                'rgb_gripper': f"{sensor_stats['rgb_gripper']}/{total_frames}",
                'depth_gripper': f"{sensor_stats['depth_gripper']}/{total_frames}",
                'rgb_tactile': f"{sensor_stats['rgb_tactile']}/{total_frames}",    # â­
                'depth_tactile': f"{sensor_stats['depth_tactile']}/{total_frames}", # â­
                'robot_obs': f"{sensor_stats['robot_obs']}/{total_frames}",
            },
            'visual_info': visual_info
        }
        
        if trajectory_data['missing_count'] > 0:
            episode_json['source_frames']['missing_frames'] = trajectory_data['missing_frames']
        
        return episode_json
    
    def process_all(self, save_images: bool = True, max_episodes: int = None):
        """å¤„ç†æ‰€æœ‰episodes"""
        print("="*70)
        print("ğŸš€ å¼€å§‹å¤„ç†Calvinæ•°æ®é›†")
        print("="*70)
        
        episodes = self.find_episode_sequences()
        if not episodes:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•episode")
            return {}
        
        print(f"\nğŸ“Š Episodesåˆ—è¡¨:")
        print("-"*70)
        for i, (ep_id, ep_info) in enumerate(episodes.items()):
            length = ep_info['end'] - ep_info['start'] + 1
            lang = ep_info['language_instruction']
            if len(lang) > 50:
                lang = lang[:47] + '...'
            print(f"{i+1:2d}. {ep_id}: å¸§ {ep_info['start']:6d}-{ep_info['end']:6d} "
                  f"({length:3d}å¸§) - {lang}")
        
        if max_episodes:
            episodes = dict(list(episodes.items())[:max_episodes])
            print(f"\nâš ï¸  é™åˆ¶å¤„ç†å‰ {max_episodes} ä¸ªepisodes")
        
        print()
        all_episodes = {}
        
        for episode_id, episode_info in tqdm(episodes.items(), desc="å¤„ç†episodes"):
            try:
                episode_json = self.process_episode(episode_id, episode_info, save_images)
                all_episodes[episode_id] = episode_json
                
                json_path = self.output_path / f'{episode_id}.json'
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(episode_json, f, indent=2, ensure_ascii=False)
                
            except Exception as e:
                print(f"\nâŒ å¤„ç† {episode_id} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # ç”Ÿæˆæ•°æ®é›†æ‘˜è¦
        summary = {
            'dataset_name': 'calvin',
            'total_episodes': len(all_episodes),
            'sensor_config': {
                'static_camera_rgb': True,
                'static_camera_depth': True,
                'gripper_camera_rgb': self.include_gripper_cam,
                'gripper_camera_depth': self.include_gripper_cam,
                'tactile_rgb': self.include_tactile,      # â­
                'tactile_depth': self.include_tactile     # â­
            },
            'episodes': list(all_episodes.keys())
        }
        
        # ç»Ÿè®¡ä¼ æ„Ÿå™¨è¦†ç›–ç‡
        if all_episodes:
            sensor_coverage_avg = {}
            for ep in all_episodes.values():
                for sensor, coverage in ep['sensor_coverage'].items():
                    available, total = map(int, coverage.split('/'))
                    if sensor not in sensor_coverage_avg:
                        sensor_coverage_avg[sensor] = []
                    sensor_coverage_avg[sensor].append(available / total if total > 0 else 0)
            
            summary['sensor_coverage_avg'] = {
                sensor: f"{np.mean(values)*100:.1f}%"
                for sensor, values in sensor_coverage_avg.items()
            }
        
        with open(self.output_path / 'dataset_summary.json', 'w') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*70}")
        print(f"âœ… å¤„ç†å®Œæˆ! æ€»episodes: {len(all_episodes)}")
        print(f"   è¾“å‡ºè·¯å¾„: {self.output_path}")
        if 'sensor_coverage_avg' in summary:
            print(f"\nğŸ“Š ä¼ æ„Ÿå™¨å¹³å‡è¦†ç›–ç‡:")
            for sensor, coverage in summary['sensor_coverage_avg'].items():
                print(f"   â€¢ {sensor}: {coverage}")
        print(f"{'='*70}")
        
        return all_episodes


def main():
    parser = argparse.ArgumentParser(
        description='Calvinæ•°æ®é›†å¤„ç†å™¨ - ä¿®æ­£ç‰ˆï¼ˆæ­£ç¡®çš„è§¦è§‰é”®åï¼‰'
    )
    
    parser.add_argument('--dataset_path', type=str, 
                       default='./calvin_debug_dataset/training',
                       help='Calvinæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_path', type=str,
                       default='./calvin_processed_with_tactile_training',
                       help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--max_episodes', type=int, default=None,
                       help='é™åˆ¶å¤„ç†çš„episodeæ•°é‡')
    
    # ä¼ æ„Ÿå™¨é€‰æ‹©
    parser.add_argument('--include_tactile', action='store_true',
                       help='åŒ…å«è§¦è§‰æ•°æ® (RGB 160x120x6 + Depth 160x120x2)')
    parser.add_argument('--no_gripper_cam', action='store_true',
                       help='ä¸åŒ…å«å¤¹çˆªç›¸æœº')
    parser.add_argument('--no_images', action='store_true',
                       help='ä¸ä¿å­˜å›¾åƒï¼ˆä»…ä¿å­˜è½¨è¿¹JSONï¼‰')
    
    args = parser.parse_args()
    
    processor = CalvinDatasetProcessor(
        args.dataset_path, 
        args.output_path,
        include_tactile=args.include_tactile,
        include_gripper_cam=not args.no_gripper_cam
    )
    
    results = processor.process_all(
        save_images=not args.no_images, 
        max_episodes=args.max_episodes
    )
    
    if results:
        print("\nğŸ“‹ ç¤ºä¾‹episode:")
        first_ep = list(results.values())[0]
        print(f"  ID: {first_ep['episode_id']}")
        print(f"  ä»»åŠ¡: {first_ep['task']}")
        print(f"  æŒ‡ä»¤: {first_ep['language_instruction']}")
        print(f"  å¸§æ•°: {first_ep['data_statistics']['num_frames']}")
        print(f"  æœºå™¨äººçŠ¶æ€ç»´åº¦: {first_ep['data_statistics']['robot_obs_dim']}")
        print(f"  åŠ¨ä½œç»´åº¦: {first_ep['data_statistics']['actions_dim']}")
        if first_ep['data_statistics'].get('has_tactile_rgb'):
            print(f"  è§¦è§‰RGBå½¢çŠ¶: {first_ep['data_statistics']['tactile_rgb_shape']}")
        if first_ep['data_statistics'].get('has_tactile_depth'):
            print(f"  è§¦è§‰æ·±åº¦å½¢çŠ¶: {first_ep['data_statistics']['tactile_depth_shape']}")
        
        print(f"\nğŸ“Š ä¼ æ„Ÿå™¨è¦†ç›–ç‡:")
        for sensor, coverage in first_ep['sensor_coverage'].items():
            print(f"  â€¢ {sensor}: {coverage}")


if __name__ == "__main__":
    main()