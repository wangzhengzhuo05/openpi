#!/usr/bin/env python3
"""
Calvinæ•°æ®é›†æ¢æµ‹å™¨ - æŸ¥çœ‹npzæ–‡ä»¶ä¸­çš„å®é™…å†…å®¹
"""

import numpy as np
from pathlib import Path
import argparse
from collections import Counter


def inspect_npz_file(npz_path: Path, verbose: bool = True):
    """è¯¦ç»†æ£€æŸ¥å•ä¸ªnpzæ–‡ä»¶"""
    try:
        data = np.load(npz_path, allow_pickle=True)
        
        if verbose:
            print(f"\n{'='*70}")
            print(f"ğŸ“‚ æ–‡ä»¶: {npz_path.name}")
            print(f"{'='*70}")
        
        file_info = {}
        
        for key in data.files:
            array = data[key]
            
            info = {
                'shape': array.shape if hasattr(array, 'shape') else 'N/A',
                'dtype': array.dtype if hasattr(array, 'dtype') else type(array),
                'size_mb': array.nbytes / 1024 / 1024 if hasattr(array, 'nbytes') else 0
            }
            
            file_info[key] = info
            
            if verbose:
                print(f"\nğŸ”‘ Key: '{key}'")
                print(f"   å½¢çŠ¶: {info['shape']}")
                print(f"   ç±»å‹: {info['dtype']}")
                print(f"   å¤§å°: {info['size_mb']:.2f} MB")
                
                # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                if hasattr(array, 'shape') and len(array.shape) > 0:
                    if array.dtype in [np.float32, np.float64, np.int32, np.int64]:
                        print(f"   èŒƒå›´: [{array.min():.4f}, {array.max():.4f}]")
                        print(f"   å‡å€¼: {array.mean():.4f}")
        
        return file_info
        
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return None


def scan_dataset(dataset_path: Path, num_samples: int = 10):
    """æ‰«ææ•°æ®é›†ï¼Œç»Ÿè®¡æ‰€æœ‰å¯ç”¨çš„é”®"""
    print(f"\n{'='*70}")
    print(f"ğŸ” æ‰«ææ•°æ®é›†: {dataset_path}")
    print(f"{'='*70}")
    
    npz_files = sorted(dataset_path.glob('episode_*.npz'))
    
    if not npz_files:
        print("âŒ æœªæ‰¾åˆ°npzæ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(npz_files)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š é‡‡æ · {min(num_samples, len(npz_files))} ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ...")
    
    # é‡‡æ ·æ–‡ä»¶
    sample_indices = np.linspace(0, len(npz_files)-1, num_samples, dtype=int)
    sample_files = [npz_files[i] for i in sample_indices]
    
    # ç»Ÿè®¡æ‰€æœ‰é”®
    all_keys = Counter()
    key_shapes = {}
    
    for npz_file in sample_files:
        try:
            data = np.load(npz_file, allow_pickle=True)
            
            for key in data.files:
                all_keys[key] += 1
                
                # è®°å½•å½¢çŠ¶
                if key not in key_shapes:
                    array = data[key]
                    key_shapes[key] = {
                        'shape': array.shape if hasattr(array, 'shape') else 'N/A',
                        'dtype': array.dtype if hasattr(array, 'dtype') else type(array)
                    }
                    
        except Exception as e:
            print(f"âš ï¸  è¯»å– {npz_file.name} å¤±è´¥: {e}")
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ•°æ®é›†å†…å®¹ç»Ÿè®¡")
    print(f"{'='*70}")
    
    print(f"\nå‘ç°çš„æ•°æ®é”®ï¼ˆæŒ‰é¢‘ç‡æ’åºï¼‰:")
    print(f"{'-'*70}")
    
    for key, count in all_keys.most_common():
        percentage = (count / len(sample_files)) * 100
        shape_info = key_shapes[key]
        
        print(f"\nğŸ”‘ '{key}'")
        print(f"   å‡ºç°é¢‘ç‡: {count}/{len(sample_files)} ({percentage:.1f}%)")
        print(f"   å½¢çŠ¶: {shape_info['shape']}")
        print(f"   ç±»å‹: {shape_info['dtype']}")
    
    # åˆ†æè§¦è§‰ç›¸å…³
    print(f"\n{'='*70}")
    print(f"ğŸ” è§¦è§‰æ•°æ®åˆ†æ")
    print(f"{'='*70}")
    
    tactile_keys = [k for k in all_keys.keys() if 'tact' in k.lower() or 'touch' in k.lower()]
    
    if tactile_keys:
        print(f"âœ… æ‰¾åˆ°è§¦è§‰ç›¸å…³é”®:")
        for key in tactile_keys:
            print(f"   â€¢ {key}: {key_shapes[key]}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°è§¦è§‰æ•°æ®")
        print(f"\nå¯èƒ½çš„åŸå› :")
        print(f"   1. æ­¤æ•°æ®é›†é…ç½®ä¸åŒ…å«è§¦è§‰ä¼ æ„Ÿå™¨")
        print(f"   2. è§¦è§‰æ•°æ®åœ¨ä¸åŒçš„æ–‡ä»¶æˆ–ä½ç½®")
        print(f"   3. éœ€è¦ç‰¹å®šçš„Calvinç¯å¢ƒç‰ˆæœ¬")
    
    # æ£€æŸ¥æ‰€æœ‰å¯ç”¨çš„ä¼ æ„Ÿå™¨
    print(f"\n{'='*70}")
    print(f"ğŸ“· å¯ç”¨ä¼ æ„Ÿå™¨æ€»ç»“")
    print(f"{'='*70}")
    
    sensors = {
        'rgb_static': 'Static Camera RGB',
        'rgb_gripper': 'Gripper Camera RGB',
        'depth_static': 'Static Camera Depth',
        'depth_gripper': 'Gripper Camera Depth',
        'robot_obs': 'Robot State',
        'scene_obs': 'Scene Objects',
        'actions': 'Actions',
        'rel_actions': 'Relative Actions'
    }
    
    for key, name in sensors.items():
        if key in all_keys:
            count = all_keys[key]
            percentage = (count / len(sample_files)) * 100
            status = "âœ…" if percentage > 90 else "âš ï¸"
            print(f"{status} {name:25s}: {count}/{len(sample_files)} ({percentage:.1f}%)")
        else:
            print(f"âŒ {name:25s}: ä¸å¯ç”¨")
    
    return all_keys, key_shapes


def compare_episodes(dataset_path: Path, ep_indices: list):
    """æ¯”è¾ƒä¸åŒepisodeçš„æ•°æ®å†…å®¹"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ æ¯”è¾ƒä¸åŒepisodeçš„å†…å®¹")
    print(f"{'='*70}")
    
    for idx in ep_indices:
        npz_file = dataset_path / f'episode_{idx:07d}.npz'
        if npz_file.exists():
            print(f"\nğŸ“„ Episode {idx:07d}:")
            inspect_npz_file(npz_file, verbose=False)
            
            data = np.load(npz_file, allow_pickle=True)
            print(f"   åŒ…å«çš„é”®: {', '.join(data.files)}")
        else:
            print(f"âŒ Episode {idx:07d} ä¸å­˜åœ¨")


def main():
    parser = argparse.ArgumentParser(
        description='Calvinæ•°æ®é›†æ¢æµ‹å™¨ - æŸ¥çœ‹å®é™…åŒ…å«çš„æ•°æ®'
    )
    
    parser.add_argument('dataset_path', type=str,
                       help='Calvinæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--inspect_file', type=str, default=None,
                       help='è¯¦ç»†æ£€æŸ¥ç‰¹å®šçš„npzæ–‡ä»¶')
    parser.add_argument('--scan_samples', type=int, default=20,
                       help='æ‰«ææ—¶é‡‡æ ·çš„æ–‡ä»¶æ•°é‡')
    parser.add_argument('--compare_episodes', type=str, default=None,
                       help='æ¯”è¾ƒå¤šä¸ªepisodeï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 0,1000,2000')
    
    args = parser.parse_args()
    
    dataset_path = Path(args.dataset_path)
    
    if not dataset_path.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return
    
    # æ¨¡å¼1: æ£€æŸ¥ç‰¹å®šæ–‡ä»¶
    if args.inspect_file:
        file_path = Path(args.inspect_file)
        if not file_path.exists():
            file_path = dataset_path / args.inspect_file
        
        if file_path.exists():
            inspect_npz_file(file_path, verbose=True)
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # æ¨¡å¼2: æ¯”è¾ƒå¤šä¸ªepisode
    if args.compare_episodes:
        indices = [int(x.strip()) for x in args.compare_episodes.split(',')]
        compare_episodes(dataset_path, indices)
        return
    
    # æ¨¡å¼3: æ‰«ææ•´ä¸ªæ•°æ®é›†ï¼ˆé»˜è®¤ï¼‰
    scan_dataset(dataset_path, args.scan_samples)
    
    # ç»™å‡ºå»ºè®®
    print(f"\n{'='*70}")
    print(f"ğŸ’¡ ä½¿ç”¨å»ºè®®")
    print(f"{'='*70}")
    print(f"\nè¯¦ç»†æ£€æŸ¥ç‰¹å®šæ–‡ä»¶:")
    print(f"  python inspect_calvin_data.py {dataset_path} --inspect_file episode_0000000.npz")
    print(f"\næ¯”è¾ƒå¤šä¸ªepisode:")
    print(f"  python inspect_calvin_data.py {dataset_path} --compare_episodes 0,1000,2000")
    print(f"\næ‰«ææ›´å¤šæ ·æœ¬:")
    print(f"  python inspect_calvin_data.py {dataset_path} --scan_samples 100")


if __name__ == '__main__':
    main()
