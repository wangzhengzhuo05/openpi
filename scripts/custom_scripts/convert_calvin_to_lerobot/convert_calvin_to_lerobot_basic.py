"""
Convert CALVIN dataset to LeRobot format for œÄ‚ÇÄ model

This version creates a dataset compatible with the œÄ‚ÇÄ model requirements:
- 3 RGB images at 224x224x3, stored as uint8 [0, 255]
  (right_wrist_0_rgb is duplicated from left_wrist_0_rgb since CALVIN only has 2 cameras)
- State vector of dimension 32 (zero-padded if original state < 32)
- Tokenized prompts for language instructions

Note: Images are stored in uint8 format. The policy transforms will handle
conversion to the model's expected format (float32 [-1, 1]) during training.

Image masks should be generated by the data transforms based on which cameras
contain real vs duplicated data.

Usage:
    python convert_calvin_to_lerobot_basic.py --data_dir /path/to/calvin_debug_processed/training
"""

import json
import shutil
from pathlib import Path
from typing import Dict, Any
import numpy as np
from PIL import Image
import tyro

from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset


def load_episode_json(json_path: Path) -> Dict[str, Any]:
    """Load episode JSON file."""
    with open(json_path, 'r') as f:
        return json.load(f)


def load_and_preprocess_image(image_path: Path, target_size: tuple = (224, 224)) -> np.ndarray:
    """
    Load image and resize to target size.
    
    Args:
        image_path: Path to image file
        target_size: Target size (height, width)
    
    Returns:
        Image array as uint8 in [0, 255] range
    """
    img = Image.open(image_path)
    img = img.resize((target_size[1], target_size[0]), Image.BILINEAR)
    img_array = np.array(img, dtype=np.uint8)
    
    return img_array


def pad_or_truncate_state(state: np.ndarray, target_dim: int = 32) -> np.ndarray:
    """
    Pad or truncate state vector to target dimension.
    
    Args:
        state: Input state vector
        target_dim: Target dimension (default 32 for œÄ‚ÇÄ)
    
    Returns:
        State vector of target dimension
    """
    current_dim = len(state)
    
    if current_dim == target_dim:
        return state
    elif current_dim < target_dim:
        # Pad with zeros
        padding = np.zeros(target_dim - current_dim, dtype=np.float32)
        return np.concatenate([state, padding])
    else:
        # Truncate
        return state[:target_dim]


def get_frame_number(filename: str) -> int:
    """Extract frame number from filename like 'frame_0358656.png'."""
    return int(filename.split('_')[1].split('.')[0])


def main(
    data_dir: str,
    *,
    repo_name: str = "Coil1987121/calvin_pi0",
    push_to_hub: bool = False,
    fps: int = 30,
    state_dim: int = 32,
    max_token_len: int = 48,
):
    """
    Convert CALVIN dataset to LeRobot format for œÄ‚ÇÄ model.
    
    Args:
        data_dir: Path to CALVIN dataset training directory
        repo_name: Repository name for the LeRobot dataset
        push_to_hub: Whether to push to Hugging Face Hub
        fps: Frames per second for the dataset
        state_dim: State vector dimension (default 32 for œÄ‚ÇÄ)
        max_token_len: Maximum token length for prompts (default 48 for œÄ‚ÇÄ)
    """
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise ValueError(f"Data directory does not exist: {data_dir}")
    
    # Clean up any existing dataset in the output directory
    output_path = HF_LEROBOT_HOME / repo_name
    if output_path.exists():
        print(f"Removing existing dataset at {output_path}")
        shutil.rmtree(output_path)
    
    # Get first episode to check data
    json_files = sorted(data_path.glob("lang_ann_*.json"))
    if not json_files:
        raise ValueError(f"No episode JSON files found in {data_dir}")
    
    print(f"Found {len(json_files)} episodes")
    first_episode = load_episode_json(json_files[0])
    
    # Check original state dimension
    original_state_dim = len(first_episode["trajectory"]["robot_observations"][0])
    print(f"Original robot state dimension: {original_state_dim}")
    print(f"Target state dimension: {state_dim}")
    if original_state_dim != state_dim:
        print(f"‚ö†Ô∏è  State will be {'padded with zeros' if original_state_dim < state_dim else 'truncated'}")
    
    # Define features for the LeRobot dataset (œÄ‚ÇÄ format)
    features = {
        # Three RGB cameras at 224x224x3
        # Note: Only base_0_rgb and left_wrist_0_rgb contain real data
        # right_wrist_0_rgb is duplicated from left_wrist_0_rgb
        "observation.images.base_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.left_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.right_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        # Robot state (dimension 32 for œÄ‚ÇÄ)
        "observation.state": {
            "dtype": "float32",
            "shape": (state_dim,),
            "names": ["state"],
        },
        # Actions (7-DOF for Franka)
        "actions": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["actions"],
        },
        # Language instruction fields (required)
        "observation.tokenized_prompt": {
            "dtype": "int32",
            "shape": (max_token_len,),
            "names": ["tokens"],
        },
        "observation.tokenized_prompt_mask": {
            "dtype": "bool",
            "shape": (max_token_len,),
            "names": ["mask"],
        },
    }
    
    print("\nCreating LeRobot dataset with features:")
    for key, value in features.items():
        print(f"  {key}: shape={value['shape']}, dtype={value['dtype']}")
    
    # Create LeRobot dataset
    dataset = LeRobotDataset.create(
        repo_id=repo_name,
        robot_type="franka",  # CALVIN uses Franka Panda robot
        fps=fps,
        features=features,
        image_writer_threads=10,
        image_writer_processes=5,
    )
    
    # Process each episode
    print(f"\nProcessing episodes...")
    for episode_idx, json_path in enumerate(json_files):
        episode_data = load_episode_json(json_path)
        episode_id = episode_data["episode_id"]
        
        print(f"\nEpisode {episode_idx + 1}/{len(json_files)}: {episode_id}")
        print(f"  Task: {episode_data['language_instruction']}")
        print(f"  Frames: {len(episode_data['trajectory']['robot_observations'])}")
        
        # Get image directory for this episode
        images_dir = data_path / "images" / episode_id
        
        # Get all frame numbers from one of the image folders
        rgb_static_dir = images_dir / "rgb_static"
        frame_files = sorted(list(rgb_static_dir.glob("*.png")))
        frame_numbers = [get_frame_number(f.name) for f in frame_files]
        
        print(f"  Image frames: {len(frame_numbers)}")
        
        # Check if trajectory length matches image frames
        num_trajectory_frames = len(episode_data['trajectory']['robot_observations'])
        if len(frame_numbers) != num_trajectory_frames:
            print(f"  ‚ö†Ô∏è  Warning: Mismatch between trajectory ({num_trajectory_frames}) and images ({len(frame_numbers)})")
            num_frames = min(len(frame_numbers), num_trajectory_frames)
            frame_numbers = frame_numbers[:num_frames]
        else:
            num_frames = num_trajectory_frames
        
        # Process each frame in the episode
        for frame_idx in range(num_frames):
            frame_num = frame_numbers[frame_idx]
            frame_name = f"frame_{frame_num:07d}"
            
            # Prepare frame data
            frame_data = {}
            
            # Load and preprocess RGB images (resize to 224x224, uint8 format)
            # Map CALVIN cameras to œÄ‚ÇÄ camera names:
            # - rgb_static -> base_0_rgb (base camera)
            # - rgb_gripper -> left_wrist_0_rgb (left wrist camera)
            # - rgb_gripper -> right_wrist_0_rgb (right wrist camera, duplicate)
            
            rgb_static_path = images_dir / "rgb_static" / f"{frame_name}.png"
            rgb_gripper_path = images_dir / "rgb_gripper" / f"{frame_name}.png"
            
            frame_data["observation.images.base_0_rgb"] = load_and_preprocess_image(rgb_static_path)
            frame_data["observation.images.left_wrist_0_rgb"] = load_and_preprocess_image(rgb_gripper_path)
            # CALVIN only has 2 cameras, so duplicate gripper camera for right wrist
            frame_data["observation.images.right_wrist_0_rgb"] = load_and_preprocess_image(rgb_gripper_path)
            
            # Get robot state and pad/truncate to target dimension
            robot_state = np.array(
                episode_data["trajectory"]["robot_observations"][frame_idx],
                dtype=np.float32
            )
            frame_data["observation.state"] = pad_or_truncate_state(robot_state, state_dim)
            
            # Add actions
            frame_data["actions"] = np.array(
                episode_data["trajectory"]["actions"][frame_idx],
                dtype=np.float32
            )
            
            # Add language instruction (required)
            # Simple tokenization: convert to ASCII codes (placeholder)
            # In practice, you should use a proper tokenizer
            instruction = episode_data["language_instruction"]
            tokens = [ord(c) for c in instruction[:max_token_len]]
            
            # Pad tokens to max_token_len
            if len(tokens) < max_token_len:
                token_mask = [True] * len(tokens) + [False] * (max_token_len - len(tokens))
                tokens = tokens + [0] * (max_token_len - len(tokens))
            else:
                token_mask = [True] * max_token_len
            
            frame_data["observation.tokenized_prompt"] = np.array(tokens, dtype=np.int32)
            frame_data["observation.tokenized_prompt_mask"] = np.array(token_mask, dtype=bool)
            
            # Add task/language instruction
            frame_data["task"] = episode_data["language_instruction"]
            
            # Add frame to dataset
            dataset.add_frame(frame_data)
        
        # Save episode
        dataset.save_episode()
        print(f"  ‚úì Episode saved ({num_frames} frames)")
    
    print(f"\n‚úÖ Conversion complete! Dataset saved to: {output_path}")
    print(f"Total episodes: {len(json_files)}")
    print(f"Total frames: {len(dataset)}")
    
    # Optionally push to Hugging Face Hub
    if push_to_hub:
        print(f"\nüì§ Pushing dataset to Hugging Face Hub: {repo_name}")
        dataset.push_to_hub(
            tags=["calvin", "franka", "manipulation", "language-conditioned", "pi0"],
            private=False,
            push_videos=True,
            license="mit",
        )
        print("‚úì Dataset pushed to Hub!")


if __name__ == "__main__":
    tyro.cli(main)