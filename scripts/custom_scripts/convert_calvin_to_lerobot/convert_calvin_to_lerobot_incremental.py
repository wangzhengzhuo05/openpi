"""
Convert CALVIN dataset to LeRobot format - INCREMENTAL VERSION with Timestamp Error Handling

æ”¯æŒå¢é‡å¤„ç†å’Œæ—¶é—´æˆ³é”™è¯¯å¤„ç†ï¼š
- ä¸åˆ é™¤å·²æœ‰æ•°æ®é›†
- è‡ªåŠ¨ä»å·²å¤„ç†çš„ä½ç½®ç»§ç»­
- å¯ä»¥å®‰å…¨ä¸­æ–­å’Œæ¢å¤
- è‡ªåŠ¨åˆ é™¤æœ‰æ—¶é—´æˆ³é”™è¯¯çš„ episodes

Usage:
    # é¦–æ¬¡å¤„ç† 0-10000
    python convert_calvin_to_lerobot_incremental.py \
        --data_dir /path/to/calvin \
        --max_episodes 10000
    
    # ç»§ç»­å¤„ç† 10000-20000ï¼ˆè‡ªåŠ¨åˆ é™¤åçš„ episodesï¼‰
    python convert_calvin_to_lerobot_incremental.py \
        --data_dir /path/to/calvin \
        --max_episodes 10000 \
        --resume True
    
    # ä½¿ç”¨æ›´å®½æ¾çš„æ—¶é—´æˆ³å®¹å·®
    python convert_calvin_to_lerobot_incremental.py \
        --data_dir /path/to/calvin \
        --resume True \
        --tolerance_s 0.1
"""

import gc
import json
import shutil
from pathlib import Path
from typing import Dict, Any
import numpy as np
from PIL import Image
import tyro

from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset


def load_episode_json(json_path: Path) -> Dict[str, Any]:
    """Load episode JSON file."""
    with open(json_path, 'r') as f:
        return json.load(f)


def load_and_preprocess_image(
    image_path: Path, 
    target_size: tuple = (224, 224),
    quality: int = 95
) -> np.ndarray:
    """Load and resize image with memory optimization."""
    img = Image.open(image_path)
    
    if img.size[0] > target_size[1] * 2 or img.size[1] > target_size[0] * 2:
        img.thumbnail((target_size[1] * 2, target_size[0] * 2), Image.LANCZOS)
    
    img = img.resize((target_size[1], target_size[0]), Image.BILINEAR)
    img_array = np.array(img, dtype=np.uint8)
    
    img.close()
    del img
    
    return img_array


def get_frame_number(filename: str) -> int:
    """Extract frame number from filename."""
    return int(filename.split('_')[1].split('.')[0])


def pad_or_truncate_state(state: np.ndarray, target_dim: int = 32) -> np.ndarray:
    """Pad or truncate state vector to target dimension."""
    current_dim = len(state)
    
    if current_dim == target_dim:
        return state
    elif current_dim < target_dim:
        padding = np.zeros(target_dim - current_dim, dtype=np.float32)
        return np.concatenate([state, padding])
    else:
        return state[:target_dim]


def get_processed_episodes(output_path: Path) -> int:
    """è·å–å·²å¤„ç†çš„ episode æ•°é‡"""
    if not output_path.exists():
        return 0
    
    # æ–¹æ³•1: ä» meta/info.json è¯»å–
    info_path = output_path / "meta" / "info.json"
    if info_path.exists():
        try:
            with open(info_path, 'r') as f:
                info = json.load(f)
                return info.get('total_episodes', 0)
        except Exception:
            pass
    
    # æ–¹æ³•2: ä» episodes ç›®å½•ç»Ÿè®¡
    episodes_dir = output_path / "meta" / "episodes"
    if episodes_dir.exists():
        episode_files = list(episodes_dir.glob("episode_*.json"))
        return len(episode_files)
    
    # æ–¹æ³•3: å°è¯•åŠ è½½æ•°æ®é›†
    try:
        from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
        dataset = LeRobotDataset(output_path.name, root=output_path.parent)
        return dataset.num_episodes
    except Exception:
        pass
    
    return 0


def remove_last_n_episodes(output_path: Path, n: int = 1):
    """
    åˆ é™¤æœ€å n ä¸ª episodesï¼ˆé€šå¸¸æ˜¯ä¸å®Œæ•´æˆ–æœ‰é—®é¢˜çš„ï¼‰
    
    è¿™ä¼šä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶ï¼š
    - data/chunk-*.parquet (åˆ é™¤æœ€åçš„è¡Œ)
    - meta/episodes/episode_*.json (åˆ é™¤æœ€åçš„æ–‡ä»¶)
    - meta/info.json (æ›´æ–° total_episodes)
    - videos/*.mp4 (åˆ é™¤å¯¹åº”çš„è§†é¢‘)
    """
    if not output_path.exists():
        print(f"Dataset path does not exist: {output_path}")
        return
    
    print(f"\nğŸ—‘ï¸  Removing last {n} episode(s) from dataset...")
    
    # 1. è·å–å½“å‰ episode æ•°é‡
    info_path = output_path / "meta" / "info.json"
    if not info_path.exists():
        print("No info.json found, cannot remove episodes")
        return
    
    with open(info_path, 'r') as f:
        info = json.load(f)
    
    total_episodes = info.get('total_episodes', 0)
    if total_episodes == 0:
        print("No episodes to remove")
        return
    
    if n >= total_episodes:
        print(f"âš ï¸  Cannot remove {n} episodes from {total_episodes} total episodes")
        print(f"   Consider using --clean_start instead")
        return
    
    episodes_to_remove = list(range(total_episodes - n, total_episodes))
    print(f"  Episodes to remove: {episodes_to_remove}")
    
    # 2. åˆ é™¤ episode JSON æ–‡ä»¶
    episodes_dir = output_path / "meta" / "episodes"
    for ep_idx in episodes_to_remove:
        ep_file = episodes_dir / f"episode_{ep_idx:06d}.json"
        if ep_file.exists():
            ep_file.unlink()
            print(f"  âœ“ Deleted {ep_file.name}")
    
    # 3. åˆ é™¤å¯¹åº”çš„è§†é¢‘æ–‡ä»¶
    videos_dir = output_path / "videos"
    if videos_dir.exists():
        for ep_idx in episodes_to_remove:
            for video_file in videos_dir.glob(f"*episode_{ep_idx:06d}*.mp4"):
                video_file.unlink()
                print(f"  âœ“ Deleted video {video_file.name}")
    
    # 4. æ›´æ–° info.json
    info['total_episodes'] = total_episodes - n
    with open(info_path, 'w') as f:
        json.dump(info, f, indent=2)
    print(f"  âœ“ Updated info.json: {total_episodes} -> {info['total_episodes']} episodes")
    
    # 5. å¤„ç† parquet æ•°æ®æ–‡ä»¶
    # æ³¨æ„ï¼šè¿™éƒ¨åˆ†æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸ºéœ€è¦æ‰¾åˆ°å¹¶åˆ é™¤ç‰¹å®š episode çš„æ‰€æœ‰å¸§
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°åŠ è½½å’Œä¿å­˜æ•°æ®é›†
    print(f"  â„¹ï¸  Note: Parquet data files need to be reprocessed")
    print(f"     The removed episodes' frames are still in the parquet files")
    print(f"     But they won't be accessed because info.json is updated")
    
    print(f"âœ… Removed {n} episode(s) successfully")


def main(
    data_dir: str = "/root/autodl-tmp/task_ABCD_D_processed/training",
    *,
    repo_name: str = "Coil1987121/calvin_lerobot_task_ABCD_D_training",
    push_to_hub: bool = False,
    include_depth: bool = False,
    include_tactile: bool = False,
    max_episodes: int | None = None,
    start_episode: int | None = None,
    state_dim: int = 32,
    fps: int = 30,
    # å¢é‡å¤„ç†å‚æ•°
    resume: bool = False,
    clean_start: bool = False,
    # æ—¶é—´æˆ³é”™è¯¯å¤„ç†å‚æ•°
    remove_bad_episodes: int = 0,  # åˆ é™¤æœ€å N ä¸ª episodesï¼ˆ0=ä¸åˆ é™¤ï¼‰
    tolerance_s: float = 1e-4,  # æ—¶é—´æˆ³å®¹å·®ï¼ˆç§’ï¼‰
    auto_fix_on_error: bool = True,  # é‡åˆ°é”™è¯¯æ—¶è‡ªåŠ¨åˆ é™¤æœ€åå‡ ä¸ª episodes
    # å†…å­˜ä¼˜åŒ–å‚æ•°
    image_quality: int = 95,
    batch_save_episodes: int = 10,
    writer_threads: int = 10,
    writer_processes: int = 4,
):
    """
    Convert CALVIN dataset to LeRobot format with incremental support and timestamp error handling.
    
    å¢é‡å¤„ç†å‚æ•°:
        resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ï¼ˆé»˜è®¤ Falseï¼‰
        clean_start: æ˜¯å¦åˆ é™¤å·²æœ‰æ•°æ®ä»å¤´å¼€å§‹ï¼ˆé»˜è®¤ Falseï¼‰
        start_episode: æ‰‹åŠ¨æŒ‡å®šèµ·å§‹ episodeï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›– resumeï¼‰
    
    æ—¶é—´æˆ³é”™è¯¯å¤„ç†:
        remove_bad_episodes: åœ¨åŠ è½½å‰åˆ é™¤æœ€å N ä¸ª episodesï¼ˆé»˜è®¤ 0ï¼‰
        tolerance_s: æ—¶é—´æˆ³æ£€æŸ¥å®¹å·®ï¼Œå¢å¤§å¯ä»¥æ”¾å®½æ£€æŸ¥ï¼ˆé»˜è®¤ 1e-4ï¼‰
        auto_fix_on_error: é‡åˆ°æ—¶é—´æˆ³é”™è¯¯æ—¶è‡ªåŠ¨åˆ é™¤é—®é¢˜ episodesï¼ˆé»˜è®¤ Trueï¼‰
    
    ä½¿ç”¨åœºæ™¯:
        1. é¦–æ¬¡å¤„ç†: 
           python script.py --max_episodes 10000
           
        2. ç»§ç»­å¤„ç†ï¼ˆè‡ªåŠ¨åˆ é™¤æœ€å1ä¸ªå¯èƒ½ä¸å®Œæ•´çš„episodeï¼‰: 
           python script.py --resume True --remove_bad_episodes 1
           
        3. é‡åˆ°æ—¶é—´æˆ³é”™è¯¯æ—¶è‡ªåŠ¨ä¿®å¤:
           python script.py --resume True --auto_fix_on_error True
           
        4. æ‰‹åŠ¨åˆ é™¤æœ€å3ä¸ªepisodesåç»§ç»­:
           python script.py --resume True --remove_bad_episodes 3
           
        5. ä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®:
           python script.py --resume True --tolerance_s 0.1
    """
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise ValueError(f"Data directory does not exist: {data_dir}")
    
    output_path = HF_LEROBOT_HOME / repo_name
    
    # å¤„ç†å¢é‡é€»è¾‘
    if clean_start and output_path.exists():
        print(f"ğŸ—‘ï¸  Clean start: Removing existing dataset at {output_path}")
        shutil.rmtree(output_path)
        dataset_exists = False
    elif output_path.exists():
        dataset_exists = True
        processed_count = get_processed_episodes(output_path)
        print(f"ğŸ“‚ Found existing dataset with {processed_count} episodes")
        
        # åˆ é™¤æŒ‡å®šæ•°é‡çš„ episodes
        if remove_bad_episodes > 0:
            remove_last_n_episodes(output_path, remove_bad_episodes)
            processed_count = get_processed_episodes(output_path)
            print(f"ğŸ“‚ After removal: {processed_count} episodes")
    else:
        dataset_exists = False
        print(f"ğŸ“‚ No existing dataset found, will create new one")
    
    # è·å–æ‰€æœ‰ episode æ–‡ä»¶
    json_files = sorted(data_path.glob("lang_ann_*.json"))
    if not json_files:
        raise ValueError(f"No episode JSON files found in {data_dir}")
    
    print(f"ğŸ“Š Total episodes available: {len(json_files)}")
    
    # ç¡®å®šèµ·å§‹ä½ç½®
    if start_episode is not None:
        actual_start = start_episode
        print(f"ğŸ¯ Manual start_episode specified: {actual_start}")
    elif resume and dataset_exists:
        actual_start = get_processed_episodes(output_path)
        print(f"â™»ï¸  Resume mode: Starting from episode {actual_start}")
    else:
        actual_start = 0
        print(f"ğŸ†• Starting from episode 0")
    
    if actual_start >= len(json_files):
        print(f"âœ… All episodes already processed! ({actual_start}/{len(json_files)})")
        return
    
    # ç¡®å®šè¦å¤„ç†çš„ episodes
    selected_files = json_files[actual_start:]
    if max_episodes:
        selected_files = selected_files[:max_episodes]
    
    print(f"ğŸ“ Will process episodes {actual_start} to {actual_start + len(selected_files)}")
    
    # Load first episode to determine dimensions
    first_episode = load_episode_json(json_files[0])
    print(f"ğŸ¤– Robot state dimension: {state_dim}")
    
    # Check tactile dimensions if enabled
    tactile_rgb_dim = 0
    tactile_depth_dim = 0
    if include_tactile:
        images_dir = data_path / "images" / first_episode["episode_id"]
        tactile_rgb_dir = images_dir / "tactile_rgb"
        if tactile_rgb_dir.exists():
            sample_files = sorted(list(tactile_rgb_dir.glob("*.npy")))
            if sample_files:
                sample_data = np.load(sample_files[0])
                tactile_rgb_dim = sample_data.size
                print(f"ğŸ‘† Tactile RGB dimension: {tactile_rgb_dim}")
                del sample_data
        
        tactile_depth_dir = images_dir / "tactile_depth"
        if tactile_depth_dir.exists():
            sample_files = sorted(list(tactile_depth_dir.glob("*.npy")))
            if sample_files:
                sample_data = np.load(sample_files[0])
                tactile_depth_dim = sample_data.size
                print(f"ğŸ‘† Tactile depth dimension: {tactile_depth_dim}")
                del sample_data
    
    total_state_dim = state_dim + tactile_rgb_dim + tactile_depth_dim
    print(f"ğŸ“ Total state dimension: {total_state_dim}")
    
    # Define features
    features = {
        "observation.images.base_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.left_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.images.right_wrist_0_rgb": {
            "dtype": "image",
            "shape": (224, 224, 3),
            "names": ["height", "width", "channel"],
        },
        "observation.state": {
            "dtype": "float32",
            "shape": (state_dim,),
            "names": ["state"],
        },
        "actions": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["actions"],
        },
    }
    
    if include_depth:
        features["observation.images.depth_static"] = {
            "dtype": "image",
            "shape": (200, 200, 3),
            "names": ["height", "width", "channel"],
        }
        features["observation.images.depth_gripper"] = {
            "dtype": "image",
            "shape": (84, 84, 3),
            "names": ["height", "width", "channel"],
        }
    
    print("\nğŸ“‹ Dataset features:")
    for key, value in features.items():
        print(f"  {key}: shape={value['shape']}, dtype={value['dtype']}")
    
    print(f"\nğŸ”§ Settings:")
    print(f"  Image quality: {image_quality}")
    print(f"  Writer threads: {writer_threads}")
    print(f"  GC batch size: {batch_save_episodes} episodes")
    print(f"  Timestamp tolerance: {tolerance_s}s")
    print(f"  Auto-fix on error: {auto_fix_on_error}")
    
    # Create or load dataset with error handling
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if dataset_exists and not clean_start:
                print(f"\nğŸ“‚ Loading existing dataset (attempt {attempt + 1}/{max_retries})...")
                dataset = LeRobotDataset(
                    repo_name, 
                    root=HF_LEROBOT_HOME / repo_name
                )
                print(f"âœ“ Loaded dataset with {dataset.num_episodes} episodes, {len(dataset)} frames")
                break
            else:
                print(f"\nğŸ†• Creating new dataset...")
                dataset = LeRobotDataset.create(
                    repo_id=repo_name,
                    robot_type="franka",
                    fps=fps,
                    features=features,
                    tolerance_s=tolerance_s,
                    image_writer_threads=writer_threads,
                    image_writer_processes=writer_processes,
                )
                print(f"âœ“ Created new dataset")
                break
                
        except ValueError as e:
            if "timestamps unexpectedly violate the tolerance" in str(e):
                print(f"\nâš ï¸  Timestamp validation error detected!")
                print(f"Error: {str(e)}")
                
                if not auto_fix_on_error:
                    print(f"\nâŒ Auto-fix is disabled. Please:")
                    print(f"   1. Run with --remove_bad_episodes N to manually remove last N episodes")
                    print(f"   2. Or run with --auto_fix_on_error True to automatically fix")
                    print(f"   3. Or use --tolerance_s 0.1 for more lenient checking")
                    raise
                
                if attempt < max_retries - 1:
                    # è‡ªåŠ¨åˆ é™¤æœ€åå‡ ä¸ª episodes å¹¶é‡è¯•
                    episodes_to_remove = min(3, get_processed_episodes(output_path))
                    print(f"\nğŸ”§ Auto-fixing: Removing last {episodes_to_remove} episode(s)...")
                    remove_last_n_episodes(output_path, episodes_to_remove)
                    print(f"   Retrying...")
                    continue
                else:
                    print(f"\nâŒ Failed after {max_retries} attempts")
                    print(f"   Please manually clean the dataset with --remove_bad_episodes N")
                    raise
            else:
                # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                raise
    
    # Process episodes
    print(f"\nğŸš€ Processing {len(selected_files)} episodes...")
    
    for idx, json_path in enumerate(selected_files):
        episode_idx = actual_start + idx
        
        # å®šæœŸåƒåœ¾å›æ”¶
        if idx > 0 and idx % batch_save_episodes == 0:
            print(f"\nğŸ—‘ï¸  Running garbage collection after {idx} episodes...")
            gc.collect()
        
        episode_data = load_episode_json(json_path)
        episode_id = episode_data["episode_id"]
        
        print(f"\nğŸ“¦ Episode {episode_idx + 1}/{len(json_files)}: {episode_id}")
        print(f"  ğŸ’¬ Task: {episode_data['language_instruction']}")
        print(f"  ğŸ¬ Frames: {len(episode_data['trajectory']['robot_observations'])}")
        
        images_dir = data_path / "images" / episode_id
        rgb_static_dir = images_dir / "rgb_static"
        frame_files = sorted(list(rgb_static_dir.glob("*.png")))
        frame_numbers = [get_frame_number(f.name) for f in frame_files]
        
        print(f"  ğŸ–¼ï¸  Image frames: {len(frame_numbers)}")
        
        num_trajectory_frames = len(episode_data['trajectory']['robot_observations'])
        if len(frame_numbers) != num_trajectory_frames:
            print(f"  âš ï¸  Warning: Mismatch between trajectory ({num_trajectory_frames}) and images ({len(frame_numbers)})")
            num_frames = min(len(frame_numbers), num_trajectory_frames)
            frame_numbers = frame_numbers[:num_frames]
        else:
            num_frames = num_trajectory_frames
        
        # Process frames
        for frame_idx in range(num_frames):
            frame_num = frame_numbers[frame_idx]
            frame_name = f"frame_{frame_num:07d}"
            
            frame_data = {}
            
            try:
                # Load RGB images
                rgb_static_path = images_dir / "rgb_static" / f"{frame_name}.png"
                rgb_gripper_path = images_dir / "rgb_gripper" / f"{frame_name}.png"
                
                frame_data["observation.images.base_0_rgb"] = load_and_preprocess_image(
                    rgb_static_path, quality=image_quality
                )
                frame_data["observation.images.left_wrist_0_rgb"] = load_and_preprocess_image(
                    rgb_gripper_path, quality=image_quality
                )
                frame_data["observation.images.right_wrist_0_rgb"] = load_and_preprocess_image(
                    rgb_gripper_path, quality=image_quality
                )
                
                # Load depth images if needed
                if include_depth:
                    depth_static_path = images_dir / "depth_static" / f"{frame_name}.png"
                    depth_gripper_path = images_dir / "depth_gripper" / f"{frame_name}.png"
                    
                    depth_static = load_and_preprocess_image(depth_static_path, quality=image_quality)
                    depth_gripper = load_and_preprocess_image(depth_gripper_path, quality=image_quality)
                    
                    if len(depth_static.shape) == 2:
                        depth_static = np.stack([depth_static] * 3, axis=-1)
                    elif depth_static.shape[-1] == 1:
                        depth_static = np.repeat(depth_static, 3, axis=-1)
                    
                    if len(depth_gripper.shape) == 2:
                        depth_gripper = np.stack([depth_gripper] * 3, axis=-1)
                    elif depth_gripper.shape[-1] == 1:
                        depth_gripper = np.repeat(depth_gripper, 3, axis=-1)
                    
                    frame_data["observation.images.depth_static"] = depth_static
                    frame_data["observation.images.depth_gripper"] = depth_gripper
                
                # Build state vector
                robot_state = np.array(
                    episode_data["trajectory"]["robot_observations"][frame_idx],
                    dtype=np.float32
                )
                
                state_components = [robot_state]
                
                # Add tactile data if needed
                if include_tactile:
                    tactile_rgb_path = images_dir / "tactile_rgb" / f"{frame_name}.npy"
                    tactile_depth_path = images_dir / "tactile_depth" / f"{frame_name}.npy"
                    
                    if tactile_rgb_path.exists():
                        tactile_rgb = np.load(tactile_rgb_path).flatten().astype(np.float32)
                        state_components.append(tactile_rgb)
                    else:
                        state_components.append(np.zeros(tactile_rgb_dim, dtype=np.float32))
                    
                    if tactile_depth_path.exists():
                        tactile_depth = np.load(tactile_depth_path).flatten().astype(np.float32)
                        state_components.append(tactile_depth)
                    else:
                        state_components.append(np.zeros(tactile_depth_dim, dtype=np.float32))
                
                frame_data["observation.state"] = pad_or_truncate_state(np.concatenate(state_components))
                frame_data["actions"] = np.array(
                    episode_data["trajectory"]["actions"][frame_idx],
                    dtype=np.float32
                )
                frame_data["task"] = episode_data["language_instruction"]
                
                # Add frame
                dataset.add_frame(frame_data)
                
            finally:
                del frame_data
                
            # æ¯50å¸§è½»é‡çº§GC
            if frame_idx > 0 and frame_idx % 50 == 0:
                gc.collect(generation=0)
        
        # Save episode and clean up
        dataset.save_episode()
        print(f"  âœ… Episode saved ({num_frames} frames)")
        
        del episode_data
        del frame_numbers
        gc.collect(generation=0)
    
    # Final cleanup
    print(f"\nğŸ—‘ï¸  Final garbage collection...")
    gc.collect()
    
    print(f"\nâœ… Conversion complete!")
    print(f"  ğŸ“Š Total episodes in dataset: {dataset.num_episodes}")
    print(f"  ğŸ¬ Total frames in dataset: {len(dataset)}")
    print(f"  ğŸ’¾ Dataset location: {output_path}")
    
    if push_to_hub:
        print(f"\nğŸ“¤ Pushing dataset to Hugging Face Hub: {repo_name}")
        dataset.push_to_hub(
            tags=["calvin", "franka", "manipulation", "language-conditioned"],
            private=False,
            push_videos=True,
            license="mit",
        )
        print("âœ… Dataset pushed to Hub!")


if __name__ == "__main__":
    tyro.cli(main)